{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea9684c",
   "metadata": {},
   "source": [
    "## Visualization of Spectograms with Labels and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710222d",
   "metadata": {},
   "source": [
    "Goals: \n",
    "- first line: spectorgam (well readable with axes describtions)\n",
    "- second line: ground-truth labels: bars from each onset to offset in color of class\n",
    "- third line: ground-truth labels: bars from each onset to offset in color of class\n",
    "\n",
    "maybe use parts of Bens code but also make it efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d59a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers.audio_utils import mel_filter_bank\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "import json\n",
    "import re\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8fec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use WhisperSegFeatureExtractor to generate LogMel Spectorgams (other melscale and norm than librosa)\n",
    "\n",
    "class WhisperSegFeatureExtractor( WhisperFeatureExtractor ):\n",
    "    def __init__(self, sr, spec_time_step, min_frequency = None, max_frequency = None, chunk_length = 30 ):\n",
    "        \n",
    "        hop_length = int( spec_time_step * sr )\n",
    "        if hop_length != spec_time_step * sr:\n",
    "            print(\"Warning: spec_time_step * sr must be an integer. Consider changing the sampling rate sr.\")\n",
    "        \n",
    "        if sr <= 32000:\n",
    "            n_fft = 512\n",
    "        elif sr <= 80000:\n",
    "            n_fft = 1024\n",
    "        elif sr <= 150000:\n",
    "            n_fft = 2048\n",
    "        elif sr <= 300000:\n",
    "            n_fft = 4096\n",
    "        else:\n",
    "            n_fft = 8192\n",
    "            \n",
    "        if min_frequency is None:\n",
    "            min_frequency = 0\n",
    "        if max_frequency is None:\n",
    "            max_frequency = sr // 2\n",
    "            \n",
    "        super().__init__(             \n",
    "            feature_size=80,\n",
    "            sampling_rate=sr,\n",
    "            hop_length=hop_length,\n",
    "            chunk_length = chunk_length,\n",
    "            n_fft=n_fft,\n",
    "            padding_value=0.0,\n",
    "            return_attention_mask=False )\n",
    "            \n",
    "        self.mel_filters = mel_filter_bank(\n",
    "            num_frequency_bins=1 + n_fft // 2,\n",
    "            num_mel_filters=80,\n",
    "            min_frequency=min_frequency,\n",
    "            max_frequency=max_frequency,\n",
    "            sampling_rate=sr,\n",
    "            norm=\"slaney\",\n",
    "            mel_scale=\"slaney\",\n",
    "        )\n",
    "            \n",
    "class SpecViewer:\n",
    "    def __init__( self,  ):\n",
    "        self.colors = [np.array(mcolors.hex2color(color_string)) for color_string in list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())][1:] # Skip the first color since it looks not so good ...\n",
    "        unique_colors = None\n",
    "        for color_arr in self.colors:\n",
    "            if unique_colors is None:\n",
    "                unique_colors = np.asarray([color_arr])\n",
    "            else:\n",
    "                if np.all( unique_colors == color_arr, axis = 1 ).sum() == 0:\n",
    "                    unique_colors = np.concatenate( [unique_colors, color_arr[np.newaxis,:]], axis = 0 )\n",
    "        self.colors = unique_colors[ unique_colors.mean(axis = 1) < 0.8, : ]\n",
    "        \n",
    "        self.cmap = cm.get_cmap(\"magma\")\n",
    "            \n",
    "    \"\"\"\"\n",
    "    The following functions are used for implement an interactive visulization function to see the spectrogram and the label\n",
    "    \"\"\"\n",
    "\n",
    "    def chunk_audio(self, audio, start_time, end_time, sr):\n",
    "        start_idx = int( start_time * sr )\n",
    "        end_idx = int( end_time * sr )\n",
    "        chunked_audio = audio[start_idx:end_idx]\n",
    "        return chunked_audio    \n",
    "\n",
    "    def chunk_label(self, label, start_time, end_time ):\n",
    "        \n",
    "        label_onset_arr = np.array(label[\"onset\"])\n",
    "        label_offset_arr = np.array(label[\"offset\"])\n",
    "        \n",
    "        intersected_indices = np.logical_and( label_onset_arr < end_time, label_offset_arr > start_time )\n",
    "        chunked_label = {\n",
    "                \"onset\": (np.maximum(label_onset_arr[intersected_indices], start_time ) - start_time).tolist(),\n",
    "                \"offset\": (np.minimum(label_offset_arr[intersected_indices], end_time ) - start_time).tolist(),\n",
    "                \"cluster\": [ label[\"cluster\"][idx] for idx in np.argwhere(intersected_indices)[:,0] ]\n",
    "            }\n",
    "        return chunked_label   \n",
    "    \n",
    "    def min_max_norm(self, im, min_value = None, max_value = None ):\n",
    "        if min_value is None:\n",
    "            min_value = im.min()\n",
    "        if max_value is None:\n",
    "            max_value = im.max()\n",
    "        return (im -  min_value ) / max( max_value - min_value, 1e-12 )\n",
    "\n",
    "    def plot_spec_and_labels(self, offset, window_size, audio, prediction, label, sr, audio_file_name, feature_extractor, precision_bits , min_spec_value, max_spec_value, xticks_step_size ):\n",
    "        \n",
    "        all_unique_clusters = sorted(list(set( list(label[\"cluster\"]) + list(prediction[\"cluster\"]) )))\n",
    "        cluster_color_mapper = {}\n",
    "        for cluster in all_unique_clusters:\n",
    "            if cluster not in cluster_color_mapper:\n",
    "                cluster_color_mapper[cluster] = self.colors[ len(cluster_color_mapper) % len(self.colors) ]\n",
    "        \n",
    "        patches = [Patch(color=color, label=cluster) for cluster, color in cluster_color_mapper.items()]\n",
    "                \n",
    "        start_time = offset\n",
    "        end_time = start_time + window_size\n",
    "        \n",
    "        audio_chunked = self.chunk_audio( audio, start_time, end_time, sr )\n",
    "        label_chunked = self.chunk_label( label, start_time, end_time )\n",
    "        prediction_chunked = self.chunk_label( prediction, start_time, end_time )\n",
    "        \n",
    "        spec = feature_extractor( audio_chunked, sampling_rate=sr, padding = \"do_not_pad\" )[\"input_features\"][0]\n",
    "                \n",
    "        ## convert spec to colorful (3 channel)\n",
    "        spec_colorful =  self.cmap(self.min_max_norm(spec,min_spec_value, max_spec_value))[:,:,:3]\n",
    "        spec_colorful = np.flipud(spec_colorful) \n",
    "        \n",
    "        spec_time_step = feature_extractor.hop_length / sr\n",
    "        spec_xticks_step_size = int(np.round( xticks_step_size / spec_time_step )) \n",
    "        spec_xticks_values = np.arange(0, spec.shape[1]+1, spec_xticks_step_size )\n",
    "        \n",
    "        # spec_xticks_labels = np.round(spec_xticks_values * spec_time_step + start_time, precision_bits) \n",
    "        xticks_format = \"%%.%df\"%(precision_bits)\n",
    "        spec_xticks_labels = [ xticks_format%(v) for v in spec_xticks_values * spec_time_step + start_time ]\n",
    "        \n",
    "        \n",
    "        spec_labels_image = np.ones( ( spec.shape[1], 3 ), dtype = np.float32 )\n",
    "        for pos in range(len(label_chunked[\"onset\"])):\n",
    "            onset_idx = int(np.round(label_chunked[\"onset\"][pos]/spec_time_step))\n",
    "            offset_idx = int(np.round(label_chunked[\"offset\"][pos]/spec_time_step)) \n",
    "            cluster = label_chunked[\"cluster\"][pos]\n",
    "            \n",
    "            ## Add a gap manually if there are two connected segments that have the same cluster but are segmented into two parts (either by human or by machine)\n",
    "            if pos + 1<len(label_chunked[\"onset\"]) and \\\n",
    "                          offset_idx == int(np.round(label_chunked[\"onset\"][pos+1]/spec_time_step)) and \\\n",
    "                          cluster == label_chunked[\"cluster\"][pos+1]:\n",
    "                offset_idx -= 1\n",
    "            \n",
    "            spec_labels_image[onset_idx:offset_idx,:] = cluster_color_mapper[cluster]\n",
    "        spec_labels_image = np.tile( spec_labels_image[np.newaxis,:,:], [40,1,1] )\n",
    "        \n",
    "        \n",
    "        spec_preds_image = np.ones( (spec.shape[1], 3), dtype = np.float32 )\n",
    "        for pos in range(len(prediction_chunked[\"onset\"])):\n",
    "            onset_idx = int(np.round(prediction_chunked[\"onset\"][pos]/spec_time_step))\n",
    "            offset_idx = int(np.round(prediction_chunked[\"offset\"][pos]/spec_time_step))\n",
    "            cluster = prediction_chunked[\"cluster\"][pos]\n",
    "            \n",
    "            if pos + 1<len(prediction_chunked[\"onset\"]) and \\\n",
    "                            offset_idx == int(np.round(prediction_chunked[\"onset\"][pos+1]/spec_time_step)) and \\\n",
    "                            cluster == prediction_chunked[\"cluster\"][pos+1]:\n",
    "                offset_idx -= 1\n",
    "            \n",
    "            spec_preds_image[onset_idx:offset_idx,:] = cluster_color_mapper[cluster]\n",
    "        spec_preds_image = np.tile( spec_preds_image[np.newaxis,:,:], [40,1,1] )\n",
    "        \n",
    "        \n",
    "        canvas_image = np.ones( ( spec_colorful.shape[0] + 10 + 40 + 10 + 40, spec_labels_image.shape[1], 3 ) )\n",
    "        canvas_image[:spec_colorful.shape[0],:,:] = spec_colorful\n",
    "        canvas_image[spec_colorful.shape[0]+10:spec_colorful.shape[0]+50,:,:] = spec_preds_image \n",
    "        canvas_image[spec_colorful.shape[0]+60:spec_colorful.shape[0]+100,:,:] = spec_labels_image\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 3), constrained_layout=True)\n",
    "        gs = fig.add_gridspec(3, 1, height_ratios=[spec.shape[0], 40, 40], hspace=0.2)\n",
    "\n",
    "\n",
    "        # Spektrogramm\n",
    "        ax_spec = fig.add_subplot(gs[0])\n",
    "        ax_spec.imshow(spec_colorful, aspect='equal', origin='upper') \n",
    "        ax_spec.set_ylabel(\"Frequency (kHz)\")\n",
    "        ax_spec.set_xticks(spec_xticks_values)\n",
    "        ax_spec.set_xticklabels(spec_xticks_labels)\n",
    "        ax_spec.set_xlabel(\"Time (s)\")\n",
    "\n",
    "        # Y-Ticks wie vorher (deine Tick-Logik hier rein!)\n",
    "        num_mel_bins = spec.shape[0]\n",
    "        mel_bin_freqs = np.linspace(sr / 2, 0, num_mel_bins)\n",
    "        tick_freqs_khz = np.arange(0, int(sr / 2 / 1000) + 1, 1)\n",
    "        tick_positions = [np.argmin(np.abs(mel_bin_freqs - f * 1000)) for f in tick_freqs_khz]\n",
    "        tick_labels = [f\"{f}\" for f in tick_freqs_khz]\n",
    "        ax_spec.set_yticks(tick_positions)\n",
    "        ax_spec.set_yticklabels(tick_labels)\n",
    "\n",
    "        # Prediction-Balken\n",
    "        ax_pred = fig.add_subplot(gs[1])\n",
    "        ax_pred.imshow(spec_preds_image, aspect='equal', origin='upper')\n",
    "        ax_pred.set_yticks([])\n",
    "        ax_pred.set_xticks([])\n",
    "        ax_pred.spines[['top', 'bottom', 'right', 'left']].set_visible(False)\n",
    "\n",
    "        # Label-Balken\n",
    "        ax_label = fig.add_subplot(gs[2])\n",
    "        ax_label.imshow(spec_labels_image, aspect='equal', origin='upper')\n",
    "        ax_label.set_xticks([]) \n",
    "        ax_label.set_yticks([])\n",
    "        ax_label.spines[['top', 'bottom', 'right', 'left']].set_visible(False)\n",
    "        \n",
    "        # Legende oben rechts\n",
    "        plt.legend(handles=patches, loc=\"upper right\", bbox_to_anchor=(1, 1))\n",
    "    \n",
    "        \n",
    "                \n",
    "    def visualize( self, audio, sr, prediction = None, label = None, min_frequency = None, max_frequency = None, precision_bits = 3, audio_file_name = \"\", window_size = 5.0, xticks_step_size = 0.5, spec_width = 1000):\n",
    "    \n",
    "        feature_extractor = WhisperSegFeatureExtractor( sr, window_size / spec_width, min_frequency, max_frequency )\n",
    "        \n",
    "        \n",
    "        whole_spec = feature_extractor( audio, sampling_rate=sr, padding = \"do_not_pad\" )[\"input_features\"][0]\n",
    "        min_spec_value = None  # np.percentile( whole_spec, 0.02)\n",
    "        max_spec_value = None  # np.percentile( whole_spec, 99.98)\n",
    "        \n",
    "        if isinstance( label, pd.DataFrame ):\n",
    "            label_dict = label.to_dict(\"list\")\n",
    "            \n",
    "        if isinstance( prediction, pd.DataFrame ):\n",
    "            prediction = prediction.to_dict(\"list\")\n",
    "        \n",
    "        if label is None:\n",
    "            label = {\"onset\":[], \"offset\":[], \"cluster\":[] }\n",
    "        if prediction is None:\n",
    "            prediction = {\"onset\":[], \"offset\":[], \"cluster\":[] }\n",
    "                \n",
    "        label[\"cluster\"] = list(map(str, label[\"cluster\"]))\n",
    "        prediction[\"cluster\"] = list(map(str, prediction[\"cluster\"]))\n",
    "        \n",
    "        return interact(self.plot_spec_and_labels, \n",
    "                    offset=(0, max(0, len(audio)/sr - window_size ), window_size / 20 ), \n",
    "                    window_size = fixed(window_size), \n",
    "                    audio = fixed(audio), \n",
    "                    prediction = fixed(prediction),\n",
    "                    label = fixed(label), \n",
    "                    sr = fixed(sr), \n",
    "                    audio_file_name = fixed(audio_file_name),\n",
    "                    feature_extractor = fixed(feature_extractor),\n",
    "                    precision_bits = fixed(precision_bits),\n",
    "                    min_spec_value = fixed(min_spec_value),\n",
    "                    max_spec_value = fixed(max_spec_value),\n",
    "                    xticks_step_size = fixed(xticks_step_size)\n",
    "                        )\n",
    "\n",
    "    \n",
    "def slice_audio_and_label( audio, label, sr, start_time, end_time ):\n",
    "    sliced_audio = audio[ int( start_time * sr ):int( end_time * sr ) ]\n",
    "    duration = len(sliced_audio) / sr\n",
    "    ## get the actual ending time\n",
    "    end_time = start_time + duration\n",
    "    \n",
    "    onsets = np.array( label[\"onset\"] )\n",
    "    offsets = np.array( label[\"offset\"] )\n",
    "    clusters = list(label[\"cluster\"])\n",
    "    \n",
    "    target_indices = np.argwhere( np.logical_and( onsets < end_time, offsets > start_time ) )[:,0]\n",
    "    \n",
    "    sliced_onsets = [ max( 0, onsets[idx] - start_time ) for idx in target_indices ]\n",
    "    sliced_offsets = [ min( offsets[idx] - start_time, end_time - start_time ) for idx in target_indices ]    \n",
    "    sliced_clusters = [ clusters[idx] for idx in target_indices ]\n",
    "    \n",
    "    sliced_label = {\n",
    "        \"onset\":sliced_onsets,\n",
    "        \"offset\":sliced_offsets,\n",
    "        \"cluster\":sliced_clusters,\n",
    "    }\n",
    "    \n",
    "    if isinstance( label, pd.DataFrame ):\n",
    "        sliced_label = pd.DataFrame( sliced_label )\n",
    "    \n",
    "    return sliced_audio, sliced_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5605ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre-grete/tmp/u17327/ipykernel_502819/1610374734.py:57: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  self.cmap = cm.get_cmap(\"magma\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0a0d6a542a4835ba3b449355f7523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1095.25, description='offset', max=2190.6073125, step=0.25), Output())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Dateien laden ---\n",
    "wav_path = \"/mnt/lustre-grete/usr/u17327/lemur_data_2call/test/(2019_03_15-12_02_11)_CSWMUW240241_0000_first.wav\"\n",
    "json_path = \"/mnt/lustre-grete/usr/u17327/lemur_data_2call/test/(2019_03_15-12_02_11)_CSWMUW240241_0000_first.json\"\n",
    "pred_json_path = \"/projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/results/evaluationresults/(2019_03_15-12_02_11)_CSWMUW240241_0000_first.jsonr\"\n",
    "\n",
    "# Audio laden\n",
    "audio, sr = librosa.load(wav_path, sr=None)\n",
    "\n",
    "# Labels aus JSON laden\n",
    "with open(json_path, \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Prüfen ob alle nötigen Keys existieren\n",
    "assert all(k in labels for k in [\"onset\", \"offset\", \"cluster\"]), \"Labels JSON muss 'onset', 'offset', 'cluster' enthalten.\"\n",
    "\n",
    "# Optional: Cluster als Strings sicherstellen\n",
    "labels[\"cluster\"] = list(map(str, labels[\"cluster\"]))\n",
    "\n",
    "# Predictions aus JSON laden\n",
    "with open(pred_json_path, \"r\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Prüfen ob alle nötigen Keys existieren\n",
    "assert all(k in predictions for k in [\"onset\", \"offset\", \"cluster\"]), \"Predictions JSON muss 'onset', 'offset', 'cluster' enthalten.\"\n",
    "\n",
    "# Optional: Cluster als Strings sicherstellen\n",
    "predictions[\"cluster\"] = list(map(str, predictions[\"cluster\"]))\n",
    "\n",
    "# --- Spektrogramm und Annotationen visualisieren ---\n",
    "viewer = SpecViewer()\n",
    "\n",
    "# Interaktive Visualisierung starten\n",
    "widget = viewer.visualize(\n",
    "    audio=audio,\n",
    "    sr=sr,\n",
    "    label=labels,\n",
    "    prediction=predictions,\n",
    "    audio_file_name=wav_path,\n",
    "    window_size=5.0,      # Zeitfenstergröße in Sekunden\n",
    "    xticks_step_size=0.5  # Schrittweite der x-Achsen-Beschriftung in Sekunden\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd8cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
