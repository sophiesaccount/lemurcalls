{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bbc26ba5",
      "metadata": {},
      "source": [
        "### Project Background\n",
        "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
        "\n",
        "The code for WhisperSeg—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942fee0d",
      "metadata": {},
      "source": [
        "### Intoduction\n",
        "\n",
        "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65179682",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9469d776",
      "metadata": {},
      "source": [
        "Biological Background and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298feaf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "how to make the "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4a60a",
      "metadata": {},
      "source": [
        "## Explaination of the Models:\n",
        "\n",
        "WhisperSeg:\n",
        "\n",
        "WhisperFormer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7313843e",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "First, run the below code to install lemurcalls as editable package with the dependancies from `pyproject.toml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb118193",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -e . "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d749893",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "72d06596",
      "metadata": {},
      "source": [
        "how to run WhisperSeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3d0878c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Using fixed codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
            "Created 609 training samples after slicing\n",
            "epoch-000:   0%|                                        | 0/152 [00:00<?, ?it/s]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "epoch-000:  99%|█████████████████████████████▊| 151/152 [16:10<00:06,  6.49s/it]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/transformers/modeling_utils.py:4034: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "epoch-000:  99%|█████████████████████████████▊| 151/152 [16:17<00:06,  6.47s/it]\n",
            "The best checkpoint on validation set is: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/whisperseg_models/checkpoint-152,\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Saved loss curve to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/whisperseg_models/final_checkpoint_20260226_175148_ct2/loss_curve.png\n",
            "All Done!\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.train \\\n",
        "  --initial_model_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/whisper_models/whisper_base \\\n",
        "  --model_folder /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/whisperseg_models \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --n_threads 1 \\\n",
        "  --num_workers 1 \\\n",
        "  --max_num_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ceb55b",
      "metadata": {},
      "source": [
        "For inference, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8bd3bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Found 21 wav files in: /mnt/lustre-grete/usr/u17327/final/audios_test\n",
            "INFO:root:Current file: [  0] /mnt/lustre-grete/usr/u17327/final/audios_test/U2025_09_18_14_04_03_595-U2025_09_18_14_05_30_972.UBN_v1.WAV\n",
            "INFO:root:Current file: [  1] /mnt/lustre-grete/usr/u17327/final/audios_test/U2025_09_18_09_31_55_122-U2025_09_18_09_33_22_499.UBN_v1.WAV\n",
            "INFO:root:Current file: [  2] /mnt/lustre-grete/usr/u17327/final/audios_test/U2024_09_10_12_16_34_369-U2024_09_10_12_18_01_445.UBN_v2.WAV\n",
            "INFO:root:Current file: [  3] /mnt/lustre-grete/usr/u17327/final/audios_test/U2025_09_18_08_34_48_865-U2025_09_18_08_36_16_243.UBN_v5.WAV\n",
            "INFO:root:Current file: [  4] /mnt/lustre-grete/usr/u17327/final/audios_test/U2025_09_24_08_49_19_335-U2025_09_24_08_50_46_712.UBN_v4.WAV\n",
            "INFO:root:Current file: [  5] /mnt/lustre-grete/usr/u17327/final/audios_test/U2024_09_03_10_04_42_203-U2024_09_03_10_06_09_580.UBN_v2.WAV\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/mnt/vast-nhr/projects/mthesis_sophie_dierks/lemurcalls/lemurcalls/whisperseg/infer.py\", line 89, in <module>\n",
            "    infer(**vars(args))\n",
            "  File \"/mnt/vast-nhr/projects/mthesis_sophie_dierks/lemurcalls/lemurcalls/whisperseg/infer.py\", line 56, in infer\n",
            "    prediction = segmenter.segment(\n",
            "  File \"/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/vast-nhr/projects/mthesis_sophie_dierks/lemurcalls/lemurcalls/whisperseg/model.py\", line 424, in segment\n",
            "    generated_text_list = self.generate_segment_text( sliced_audios_features, batch_size, max_length, num_beams, top_k, top_p, length_penalty, status_monitor )\n",
            "  File \"/mnt/vast-nhr/projects/mthesis_sophie_dierks/lemurcalls/lemurcalls/whisperseg/model.py\", line 191, in generate_segment_text\n",
            "    t.join()\n",
            "  File \"/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'o' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m lemurcalls.whisperseg.infer  -d /mnt/lustre-grete/usr/u17327/final/audios_test  -m /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;241m-\u001b[39m\u001b[43mo\u001b[49m \u001b[38;5;241m/\u001b[39mprojects\u001b[38;5;241m/\u001b[39mextern\u001b[38;5;241m/\u001b[39mCIDAS\u001b[38;5;241m/\u001b[39mcidas_digitalisierung_lehre\u001b[38;5;241m/\u001b[39mmthesis_sophie_dierks\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m.\u001b[39mproject\u001b[38;5;241m/\u001b[39mlemurcalls\u001b[38;5;241m/\u001b[39mlemurcalls\u001b[38;5;241m/\u001b[39mmodel_folder_ben\u001b[38;5;241m/\u001b[39mfinal_checkpoint_20251116_163404_ct2\n",
            "\u001b[0;31mNameError\u001b[0m: name 'o' is not defined"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.infer \\\n",
        "-d /mnt/lustre-grete/usr/u17327/final/audios_single \\\n",
        "-m /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\n",
        "-o /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5daae753",
      "metadata": {},
      "outputs": [],
      "source": [
        "how to run WhisperFormer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8137b8a",
      "metadata": {},
      "source": [
        "## Thresholds and visualization\n",
        "\n",
        "If the aim is to only detect high-quality calls from the focal animal, it can be usefull to apply postprocessing filters, such as SNR and amplitude filters.\n",
        "\n",
        "-Explaination of SNR that I picked"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa592e12",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "To determine the appropriate thresholds for your dataset, you can plot the SNR and maximale amplitudes of the calls in your testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967f77df",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/best_models_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/best_models/final_model_20251205_030535/snr_amplitude_plot \\\n",
        "  --threshold 0.6 \\\n",
        "  --iou_threshold 0.4 \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec937209",
      "metadata": {},
      "source": [
        "To filter existing outputs use:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e866d0f",
      "metadata": {},
      "source": [
        "## Visualization of model outputs\n",
        "To visualize the output of the model, you can use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a865e906",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.visualize_predictions \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --pred_label_folder /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-20_11-48-55 \\\n",
        "  --output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561f88fe",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50b759",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16754d83",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
