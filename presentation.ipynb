{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3c8893",
   "metadata": {},
   "source": [
    "## Documentation of 'Lemurcalls'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfbd32",
   "metadata": {},
   "source": [
    "### Project Background\n",
    "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
    "\n",
    "The code for [WhisperSeg]—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming.\n",
    "\n",
    "For more details on the project see `thesis.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b526b",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. \n",
    "\n",
    "With this code, you can train two types of models that both detect, segment and classify lemur calls from long audio recordings. The library also enables you to assess the performance of the models.\n",
    "To further improve results,  the model predictions can be filtered by applying SNR and maximal amplitude thresholds. To determine those thresholds, the SNR and amplitude values for the calls in the training set can be plotted. To analyse model performance, the points can be colored by the predicted confidence scores. \n",
    "Further, you can analyse and compare the model predictions on the test set, by plotting the spectrograms as calculated by Whisper, the final predicted calls and, for WhisperFormer, the frame-wise confidence scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a8402",
   "metadata": {},
   "source": [
    "### Library Structure:\n",
    "\n",
    "lemurcalls is organized into two main model subpackages, lemurcalls.whisperseg and lemurcalls.whisperformer, each covering training, inference, and evaluation workflows.\n",
    "Shared data handling and utility logic is centralized in common helper modules so both pipelines use consistent preprocessing and label handling.\n",
    "In addition, the library provides visualization and postprocessing tools (e.g., SNR/amplitude filtering and precision-recall analysis) to systematically inspect and improve model outputs. The project also includes two Jupyter notebooks for easy, web-application-friendly usage (e.g., for biology bachelor students). Unit tests are available in \\tests\\.\n",
    "A more detailed overview of the library structure is shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38999b",
   "metadata": {},
   "source": [
    "```text\n",
    "lemurcalls/\n",
    "├── README.md\n",
    "├── pyproject.toml\n",
    "├── presentation_notebook/\n",
    "│   └── presentation.ipynb\n",
    "└── lemurcalls/\n",
    "    ├── __init__.py\n",
    "    ├── datautils.py\n",
    "    ├── audio_utils.py\n",
    "    ├── utils.py\n",
    "    ├── visualize_predictions.py\n",
    "    ├── download_whipser.py\n",
    "    ├── whisperseg/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── model.py\n",
    "    │   ├── train.py\n",
    "    │   ├── infer.py\n",
    "    │   ├── infer_folder.py\n",
    "    │   ├── evaluate.py\n",
    "    │   ├── evaluate_metrics.py\n",
    "    │   ├── training_utils.py\n",
    "    │   ├── datautils_ben.py\n",
    "    │   ├── utils.py\n",
    "    │   └── convert_hf_to_ct2.py\n",
    "    └── whisperformer/\n",
    "        ├── __init__.py\n",
    "        ├── model.py\n",
    "        ├── dataset.py\n",
    "        ├── datautils.py\n",
    "        ├── losses.py\n",
    "        ├── train.py\n",
    "        ├── infer.py\n",
    "        ├── postprocessing/\n",
    "        │   ├── prec_rec.py\n",
    "        │   └── filter_labels_by_snr.py\n",
    "        └── visualization/\n",
    "            └── scatterplot_ampl_snr_score.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04d4eb",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "The dataset consists of `.wav` audio recordings collected at the Affenwald Strausberg Park (Thuringia, Germany) with a total length of 4.8 hours. Each audio was resampled to 16 kHz. \n",
    "For data acquisition, ring-tailed lemurs were equipped with collars containing microphones.\n",
    "\n",
    "Training data were manually annotated in Raven Pro. For each detected call, annotators assigned onset time, offset time, and a class label.  \n",
    "Only three call types similar to the target call `moan` (see [Macedonia]) were included: `moan`, `wail`, and `hmm`.\n",
    "\n",
    "The recordings contain substantial background noise (e.g., visitor voices, nearby road traffic). In addition, microphones worn by one individual often capture calls from nearby conspecifics.  \n",
    "To account for this variability, each labeled call was assigned one of three quality classes:\n",
    "\n",
    "1. **Quality 1**: loud, high-quality calls that very likely originate from the focal individual.  \n",
    "2. **Quality 2**: medium-quality calls that likely originate from non-focal individuals.  \n",
    "3. **Quality 3**: low-quality background calls, including very quiet or distant vocalizations.\n",
    "\n",
    "Annotated labels are stored as `.json` files with the structure:  \n",
    "`{ \"onset\": [], \"offset\": [], \"cluster\": [], \"quality\": [] }`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffdffb",
   "metadata": {},
   "source": [
    "### Explanation of the Models\n",
    "\n",
    "#### WhisperSeg\n",
    "\n",
    "[WhisperSeg] (Gu et al.) builds on the pretrained [Whisper] Transformer, an automatic speech recognition model trained on 680,000 hours of multilingual supervised speech data.  \n",
    "The authors show that Whisper can be adapted effectively for animal sound event detection and classification across multiple species, and they provide a multi-species checkpoint.  \n",
    "As a sequence-to-sequence model, WhisperSeg predicts onset, offset, and class labels for detected calls.\n",
    "\n",
    "#### WhisperFormer\n",
    "\n",
    "In contrast to WhisperSeg’s token-generation objective, WhisperFormer directly predicts call centers and regresses onset/offset boundaries.  \n",
    "This center-based formulation is inspired by object detection and temporal action localization (TAL), especially [CenterNet] and [ActionFormer].\n",
    "\n",
    "Architecturally, WhisperFormer uses the Whisper encoder, followed by a lightweight decoder and two task-specific heads:\n",
    "- a **classification head** for class confidence,\n",
    "- a **regression head** for temporal boundaries.\n",
    "\n",
    "Following [ActionFormer], training combines [sigmoid] focal loss for classification and [DIoU] loss for regression:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}}(CL, S, \\tilde{CL}, \\tilde{S})\n",
    "= \\frac{1}{T_{+}}\n",
    "\\left(\n",
    "\\mathcal{L}_{\\text{class}}(CL, \\tilde{CL})\n",
    "+ \\lambda \\,\\mathbf{1}_{\\mathcal{T}_{+}} \\,\\mathcal{L}_{\\text{reg}}(S, \\tilde{S})\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Here, \\(CL\\) and \\(\\tilde{CL}\\) denote ground-truth and predicted classes, and \\(S\\) and \\(\\tilde{S}\\) denote ground-truth and predicted temporal segments.  \n",
    "At inference time, WhisperFormer outputs per-frame class confidence scores and relative onset/offset values; final detections are produced using confidence thresholding and non-maximum suppression ([NMS])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4a60a",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluation Metrics:\n",
    "\n",
    "In line with standard practice in detection tasks, we use the F1 score as the primary evaluation metric for all calls. A prediction is matched with a ground truth call, when their IoU is above a set threshold. Let $TP$ denote the number of true positives, $FP$ the number of false positives, $FN$ the number of false negatives, and $FC$ the number of predictions that match the temporal location of a call but are assigned an incorrect class. Then, precision and recall are defined as:\n",
    "$$precision = \\frac{TP}{TP + FP + FC}$$\n",
    "and \n",
    "$$recall = \\frac{TP}{TP+FN+FC}.$$\n",
    "Precision measures the accuracy of the predicted calls, while recall quantifies the proportion of ground truth calls correctly detected. The F1 score is the harmonic mean of precision and recall, balancing these two aspects:\n",
    "$$F1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}.$$\n",
    "\n",
    "\n",
    "##### Evaluation by Quality Classes\n",
    "To account for differences in call quality, we calculate F1 scores with respect to the ground truth quality classes. We distinguish the following metrics:\n",
    "1.  $F1_{Q1,Q2,Q3}$: F1 score calculated with respect to the ground truth labels of all quality classes 1,2 and 3. \n",
    "2. $F1_{Q1,Q2}$: F1 score calculated with respect to the ground truth labels of calls from quality classes 1 and 2. \n",
    "3. $F1_{Q1}$: F1 score calculated with respect to the ground truth labels of quality classes 1 only.\n",
    "\n",
    "Formally, for any subset of quality classes, the F1 score is computed using the aove formular, with precision and recall restricted to the selected quality classes.\n",
    "\n",
    "A limitation of this approach is that, when focusing on detecting high-quality calls (quality class 1), false positives from lower-quality classes (Q2 or Q3) may be less bad than completely missing detections. To address this, we define adjusted F1 metrics:\n",
    "1. $F1_{Q1,(Q1,Q3)}$: F1 score calculated with respect to the ground truth labels of quality class 1, but FP from q2 and q3 are counted as neither TPs nor FPs.\n",
    "2. $F1_{Q1,(Q2,Q3)}$: F1 score calculated with respect to the ground truth labels of quality classes 1 and 2, but FP from q3 are counted as neither TPs nor FPs.\n",
    "\n",
    "In these adjusted metrics, $TP$, $FN$, and $FC$ remain unchanged compared to $F1_{Q1}$ or $F1_{Q1,Q2}$, but the number of false positives may decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313843e",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "First, run the below code to install lemurcalls as editable package with the dependancies from `pyproject.toml` plus testing and linting tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c535e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///mnt/vast-nhr/projects/mthesis_sophie_dierks/lemurcalls\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.2 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (2.2.2+cu118)\n",
      "Requirement already satisfied: transformers>=4.40 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (4.56.1)\n",
      "Requirement already satisfied: ctranslate2>=4.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: librosa>=0.11 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.11 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (1.11.0)\n",
      "Requirement already satisfied: soundfile>=0.13 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (0.13.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=1.5 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: pandas>=2.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (3.10.1)\n",
      "Requirement already satisfied: ipywidgets>=8.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: seaborn>=0.13 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.60 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: pillow>=10.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (10.3.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.20 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (0.34.4)\n",
      "Requirement already satisfied: wandb>=0.16 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (0.20.1)\n",
      "Requirement already satisfied: pytest>=8.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from lemurcalls==0.1.0) (9.0.2)\n",
      "Collecting ruff>=0.4 (from lemurcalls==0.1.0)\n",
      "  Downloading ruff-0.15.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: setuptools in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ctranslate2>=4.0->lemurcalls==0.1.0) (80.9.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ctranslate2>=4.0->lemurcalls==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: filelock in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (24.2)\n",
      "Requirement already satisfied: requests in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from huggingface_hub>=0.20->lemurcalls==0.1.0) (1.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipywidgets>=8.0->lemurcalls==0.1.0) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipywidgets>=8.0->lemurcalls==0.1.0) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipywidgets>=8.0->lemurcalls==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipywidgets>=8.0->lemurcalls==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipywidgets>=8.0->lemurcalls==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: decorator in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.8.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (0.61.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from librosa>=0.11->lemurcalls==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from matplotlib>=3.8->lemurcalls==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.11->lemurcalls==0.1.0) (0.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pandas>=2.0->lemurcalls==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pandas>=2.0->lemurcalls==0.1.0) (2025.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.11->lemurcalls==0.1.0) (4.3.8)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pytest>=8.0->lemurcalls==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pytest>=8.0->lemurcalls==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: tomli>=1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pytest>=8.0->lemurcalls==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.8->lemurcalls==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from requests->huggingface_hub>=0.20->lemurcalls==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from requests->huggingface_hub>=0.20->lemurcalls==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from requests->huggingface_hub>=0.20->lemurcalls==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from requests->huggingface_hub>=0.20->lemurcalls==0.1.0) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from scikit-learn>=1.5->lemurcalls==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from soundfile>=0.13->lemurcalls==0.1.0) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.13->lemurcalls==0.1.0) (2.22)\n",
      "Requirement already satisfied: sympy in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (11.8.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from torch>=2.2->lemurcalls==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from transformers>=4.40->lemurcalls==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from transformers>=4.40->lemurcalls==0.1.0) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from transformers>=4.40->lemurcalls==0.1.0) (0.5.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (3.1.44)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (6.31.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (2.11.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from wandb>=0.16->lemurcalls==0.1.0) (1.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pydantic<3->wandb>=0.16->lemurcalls==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pydantic<3->wandb>=0.16->lemurcalls==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from pydantic<3->wandb>=0.16->lemurcalls==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16->lemurcalls==0.1.0) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16->lemurcalls==0.1.0) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from jinja2->torch>=2.2->lemurcalls==0.1.0) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0->lemurcalls==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages (from sympy->torch>=2.2->lemurcalls==0.1.0) (1.3.0)\n",
      "Downloading ruff-0.15.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lemurcalls\n",
      "  Building editable for lemurcalls (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lemurcalls: filename=lemurcalls-0.1.0-0.editable-py3-none-any.whl size=37459 sha256=385518f365e33f7019eff696cefdff4e88e543a3d92d908e0a37f48a411d5341\n",
      "  Stored in directory: /mnt/lustre-grete/tmp/u17327/pip-ephem-wheel-cache-1qn8_bhs/wheels/62/86/cf/193dc960836e1fff148bf05fab9d969ba89ade3ab23d14991d\n",
      "Successfully built lemurcalls\n",
      "Installing collected packages: ruff, lemurcalls\n",
      "\u001b[2K  Attempting uninstall: lemurcalls━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [ruff]\n",
      "\u001b[2K    Found existing installation: lemurcalls 0.1.0\u001b[32m0/2\u001b[0m [ruff]\n",
      "\u001b[2K    Uninstalling lemurcalls-0.1.0:━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [ruff]\n",
      "\u001b[2K      Successfully uninstalled lemurcalls-0.1.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [lemurcalls]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [lemurcalls]2\u001b[0m [lemurcalls]\n",
      "\u001b[1A\u001b[2KSuccessfully installed lemurcalls-0.1.0 ruff-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[dev]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9cad5e",
   "metadata": {},
   "source": [
    "The library includes two subpackages, `lemurcalls.whisperseg` and `lemurcalls.whisperformer`, as well as tools to visualize and compare predictions produced by trained models.  \n",
    "In the following, we define demo paths to showcase the main functionality of the Python package. Since training large models typically requires a GPU, the demonstration uses a small maximum number of epochs and a single audio file for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d749893",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = \"/projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls\"\n",
    "AUDIO_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_test\"\n",
    "AUDIO_SINGLE_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_single\"\n",
    "LABEL_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/jsons_test\"\n",
    "\n",
    "WHISPER_BASE_PATH = f\"{PROJECT_ROOT}/whisper_models/whisper_base\"\n",
    "WHISPERSEG_TRAIN_OUT = f\"{PROJECT_ROOT}/lemurcalls/whisperseg_models\"\n",
    "WHISPERSEG_MODEL_DIR = (\n",
    "    f\"{PROJECT_ROOT}/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\"\n",
    ")\n",
    "\n",
    "WHISPERFORMER_CKPT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth\"\n",
    "WHISPERFORMER_PRED_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-20_11-48-55\"\n",
    "WHISPERFORMER_VIS_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/visualization\"\n",
    "WHISPERFORMER_SNR_OUT = \"/mnt/lustre-grete/usr/u17327/final/jsons_test_filtered\"\n",
    "\n",
    "WHISPERFORMER_PREC_REC_OUT = (\n",
    "    f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/prec_rec\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d06596",
   "metadata": {},
   "source": [
    "### The WhisperSeg Subpackage\n",
    "\n",
    "To train a whisperseg model, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0878c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Using fixed codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
      "Created 609 training samples after slicing\n",
      "epoch-000:   0%|                                        | 0/152 [00:00<?, ?it/s]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "epoch-000:  99%|█████████████████████████████▊| 151/152 [14:18<00:05,  5.76s/it]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/transformers/modeling_utils.py:4034: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "epoch-000:  99%|█████████████████████████████▊| 151/152 [14:24<00:05,  5.73s/it]\n",
      "The best checkpoint on validation set is: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/whisperseg_models/checkpoint-152,\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Saved loss curve to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/whisperseg_models/final_checkpoint_20260228_002955_ct2/loss_curve.png\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m lemurcalls.whisperseg.train \\\n",
    "  --initial_model_path \"{WHISPER_BASE_PATH}\" \\\n",
    "  --model_folder \"{WHISPERSEG_TRAIN_OUT}\" \\\n",
    "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --num_classes 3 \\\n",
    "  --batch_size 4 \\\n",
    "  --n_threads 1 \\\n",
    "  --num_workers 1 \\\n",
    "  --max_num_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ceb55b",
   "metadata": {},
   "source": [
    "For inference, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8bd3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Found 1 wav files in: /mnt/lustre-grete/usr/u17327/final/audios_single\n",
      "INFO:root:Current file: [  0] /mnt/lustre-grete/usr/u17327/final/audios_single/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.WAV\n"
     ]
    }
   ],
   "source": [
    "!python -m lemurcalls.whisperseg.infer \\\n",
    "  -d \"{AUDIO_SINGLE_DIR}\" \\\n",
    "  -m \"{WHISPERSEG_MODEL_DIR}\" \\\n",
    "  -o \"{WHISPERSEG_MODEL_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc52565",
   "metadata": {},
   "source": [
    "For evaluation, you can use the original WhisperSeg evaluation.py or use evaluate_metrics.py (that is also applicable for WhisperFormer), that can be run via"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5101c",
   "metadata": {},
   "source": [
    "!python -m lemurcalls.whisperseg.evaluate_metrics \\\n",
    "  --labels \"/mnt/lustre-grete/usr/u17327/final/jsons_test/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.json\" \\\n",
    "  --predictions \"/path/to/your_prediction_file.json\" \\\n",
    "  --overlap_tolerance 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0928a3",
   "metadata": {},
   "source": [
    "Example output of `evaluate_metrics.py` for WhisperSeg model trained on high-quality data:\n",
    "```\n",
    "TP: 38\n",
    "FP: 2\n",
    "FN: 3\n",
    "FC: 1\n",
    "num gt positives: 42\n",
    "num predicted positives: 41\n",
    "Precision: 0.9268\n",
    "Recall:    0.9048\n",
    "F1-Score:  0.9157\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daae753",
   "metadata": {},
   "source": [
    "### The WhisperFormer Subpackage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b1abb",
   "metadata": {},
   "source": [
    "To train a WhisperFormer model, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9484e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  3.35it/s]\n",
      "Using codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
      "ID to cluster mapping: {0: 'm', 1: 'h', 2: 'w'}\n",
      "Created 3 training samples after slicing\n",
      "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "\n",
      "=== Starting Epoch 0 ===\n",
      "epoch-000:   0%|                                          | 0/3 [00:00<?, ?it/s]Epoch 0, Step 0, Training Total Loss: 8.8727\n",
      "Epoch 0, Step 0, Training Class Loss: 7.7938\n",
      "Epoch 0, Step 0, Training Regression Loss: 1.0789\n",
      "epoch-000: 100%|██████████████████████████████████| 3/3 [01:36<00:00, 32.20s/it]\n",
      "=== End of Epoch 0 ===\n",
      "Epoch 0, Step 2, Epoch Training Loss: 8.0226\n",
      "val_ratio = 0, will run validation: False\n",
      "No validation set (val_ratio = 0)\n",
      "\n",
      "=== Starting Epoch 1 ===\n",
      "epoch-001:   0%|                                          | 0/3 [00:00<?, ?it/s]Epoch 1, Step 0, Training Total Loss: 0.0000\n",
      "Epoch 1, Step 0, Training Class Loss: 0.0000\n",
      "Epoch 1, Step 0, Training Regression Loss: 0.0000\n",
      "epoch-001: 100%|██████████████████████████████████| 3/3 [01:38<00:00, 32.89s/it]\n",
      "=== End of Epoch 1 ===\n",
      "Epoch 1, Step 2, Epoch Training Loss: 1.2120\n",
      "val_ratio = 0, will run validation: False\n",
      "No validation set (val_ratio = 0)\n",
      "Saved loss curve to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20260228_005531/loss_curve.png\n",
      "Best model saved according to early stopping.\n",
      "✅ Best validation metrics saved to /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20260228_005531/best_val_metrics.json\n",
      "Run wurde in /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/runs.csv protokolliert.\n",
      "✅ Best metrics also appended to /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/runs.csv\n"
     ]
    }
   ],
   "source": [
    "!python -m lemurcalls.whisperformer.train \\\n",
    "  --model_folder \"{PROJECT_ROOT}/lemurcalls/model_folder_new\" \\\n",
    "  --audio_folder \"{AUDIO_SINGLE_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --num_classes 3 \\\n",
    "  --batch_size 1 \\\n",
    "  --max_num_epochs 1 \\\n",
    "  --whisper_size large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd5395",
   "metadata": {},
   "source": [
    "For inference with a set confidence score threshold, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4acc6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: whisper_size=large, num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
      "\n",
      "===== Processing U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.WAV =====\n",
      "[Run 1] Created 3 slices with offset 0\n",
      "[Run 2] Created 3 slices with offset 1000\n",
      "[Run 3] Created 3 slices with offset 2000\n",
      "✅ Predictions saved to /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-28_00-56-05/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2_preds.json\n"
     ]
    }
   ],
   "source": [
    "!python -m lemurcalls.whisperformer.infer \\\n",
    "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
    "  --audio_folder \"{AUDIO_SINGLE_DIR}\" \\\n",
    "  --output_dir \"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/sc\" \\\n",
    "  --batch_size 4 \\\n",
    "  --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56838a9",
   "metadata": {},
   "source": [
    "To identify a suitable confidence threshold and assess overall model behavior, compute a precision-recall curve across multiple score thresholds.  \n",
    "This helps you select a threshold that matches your objective (e.g., higher precision to reduce false positives, or higher recall to miss fewer calls).  \n",
    "The plot also highlights the threshold that achieves the highest $F_1$ score.\n",
    "\n",
    "You can control which label quality classes are considered via `eval_mode`:\n",
    "\n",
    "- `standard`: evaluates metrics for the selected quality set (e.g., $F_{1}$, $F_{1,2}$, or $F_{1,2,3}$).\n",
    "- `q3_q2`: applies the quality-aware evaluation strategy, where quality classes 2 and 3 are treated differently from class 1 (e.g.$F_{1,(2,3)})."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fbcab",
   "metadata": {},
   "source": [
    "```python\n",
    "!python -m lemurcalls.whisperformer.postprocessing.prec_rec \\\n",
    "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --pred_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
    "  --output_dir \"{WHISPERFORMER_PREC_REC_OUT}\" \\\n",
    "  --overlap_tolerance 0.3 \\\n",
    "  --allowed_qualities 1 \\\n",
    "  --eval_mode standard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca71a90",
   "metadata": {},
   "source": [
    "![PrecRec](presentation_notebook/precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8137b8a",
   "metadata": {},
   "source": [
    "### Thresholds and visualization\n",
    "\n",
    "If the aim is to only detect high-quality calls from the focal animal - as in our application where the goal is in the future to equip each individual with its own collar - it can be usefull to apply postprocessing filters, such as Signal-to_Noise Ratio (SNR) and amplitude filters.\n",
    "\n",
    "To determine suitable thresholds for your dataset, you can plot the SNR and maximum amplitude of calls in the training set and color the points by quality class.\n",
    "You can also color the points by the confidence scores produced by a final trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f77df",
   "metadata": {},
   "source": [
    "```python\n",
    "!python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score \\\n",
    "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
    "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --output_dir \"{WHISPERFORMER_VIS_OUT}\" \\\n",
    "  --num_classes 3 \\\n",
    "  --batch_size 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3ff16",
   "metadata": {},
   "source": [
    "![SNR vs Amplitude (Quality)](presentation_notebook/scatter_snr_vs_amplitude.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b49095",
   "metadata": {},
   "source": [
    "We can see in our dataset when we plot SNR and maximal amplitude of the labeled calls, that quality class 2 and quality class 3 cannot be (linearly) seperated easily. Quality class 1 on the other hand can - with the chosen thresholds - be seperated rather well from the other quality classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dc3c9",
   "metadata": {},
   "source": [
    "![SNR vs Amplitude (Model Score)](presentation_notebook/scatter_snr_vs_amplitude_model_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec937209",
   "metadata": {},
   "source": [
    "To filter existing predictions by SNR and maximal amplitude use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b996781",
   "metadata": {},
   "source": [
    "```python\n",
    "!python -m lemurcalls.whisperformer.postprocessing.filter_labels_by_snr \\\n",
    "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --output_dir \"{WHISPERFORMER_SNR_OUT}\" \\\n",
    "  --snr_threshold -1 \\\n",
    "  --amplitude_threshold 0.035\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e866d0f",
   "metadata": {},
   "source": [
    "### Visualization of model outputs\n",
    "To visualize, analyze and compare the predictions of two trained models, you can use the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865e906",
   "metadata": {},
   "source": [
    "```python\n",
    "!python -m lemurcalls.visualize_predictions \\\n",
    "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
    "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
    "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
    "  --pred_label_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
    "  --output_dir \"{WHISPERFORMER_VIS_OUT}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c1fac",
   "metadata": {},
   "source": [
    "The audio recordings are split into 30-second segments. For each segment, a figure with the following four rows is generated:\n",
    "\n",
    "1. Mel spectrogram computed by the original Whisper encoder.  \n",
    "2. WhisperFormer confidence scores for each spectrogram column and call class; final predictions above the selected threshold (red dashed line) are shown rectangles colored by predicted call class.  \n",
    "3. Ground-truth labels colored by call class, together with their assigned quality classes.  \n",
    "4. WhisperSeg predictions colored by predicted call class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a08bf",
   "metadata": {},
   "source": [
    "#### Output examples generated using models trained only on high-quality calls (Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7998a3",
   "metadata": {},
   "source": [
    "![Spec2](presentation_notebook/U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_01_spectrogram_scores_gt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00ebf0",
   "metadata": {},
   "source": [
    "The visualization above highlights a common WhisperFormer issue: because NMS is applied per class, and because `moan` and `wail` are inherently difficult to distinguish, both class scores may exceed the threshold for the same event. This can lead to duplicate predictions for a single ground-truth call.  \n",
    "For WhisperSeg, this example shows that very short calls can also be detected even when they are not present in the ground-truth labels, likely due to the higher temporal resolution of WhisperSeg spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa1809",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02aa952",
   "metadata": {},
   "source": [
    "![Spec4](presentation_notebook/U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_02_spectrogram_scores_gt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0269fb",
   "metadata": {},
   "source": [
    "The visualization above shows strong performance by both models. WhisperSeg produces one false positive wail; based on visual inspection of the spectrogram, this may indicate an annotation error and should be manually re-evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204131e8",
   "metadata": {},
   "source": [
    "#### Output examples generated using models trained on all quality classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c042f",
   "metadata": {},
   "source": [
    "![Spec6](presentation_notebook/U2024_09_24_12_06_37_702-U2024_09_24_12_08_05_079.UBN_v2_segment_00_spectrogram_scores_gt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62356ef8",
   "metadata": {},
   "source": [
    "Here, we can see that when trained on all data, WhisperFormer is more sensitive than WhisperSeg. Further, calls labeled as quality classes 1 and 2 receive higher WhisperFormer confidence scores than calls from quality class 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791e0d9",
   "metadata": {},
   "source": [
    "![Spec9](presentation_notebook/U2024_09_24_12_24_06_562-U2024_09_24_12_25_33_938.UBN_v2_segment_00_spectrogram_scores_gt_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20e1ef",
   "metadata": {},
   "source": [
    "The visualization above shows that both models struggle to correctly detect highly overlapping calls. WhisperFormer confidence scores suggest that they contain additional information that could potentially be used to refine the final onset and offset predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f88fe",
   "metadata": {},
   "source": [
    "![Spec5](presentation_notebook/U2025_09_24_12_59_26_005-U2025_09_24_13_00_54_382.UBN_v1_segment_02_spectrogram_scores_gt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91379d43",
   "metadata": {},
   "source": [
    "Here, we can see that the selected confidence threshold for WhisperFormer may be too low, as many unlabeled calls are detected. This example also highlights how difficult it is to consistently annotate faint, low-quality background calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84924ef",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "With this codebase, we showed that both WhisperSeg and WhisperFormer can be trained to automatically detect, segment, and classify lemur calls in long audio recordings, yielding promising results—especially when trained on high-quality data only.  \n",
    "Using dedicated visualizations, we identified suitable SNR and amplitude thresholds to improve model performance and used precision-recall curves to select an appropriate confidence threshold for WhisperFormer. Additional visual analyses were used to compare model behavior: when trained on data from all quality classes, WhisperFormer appears more sensitive than WhisperSeg, but it also tends to detect very faint calls and occasional background noise.  \n",
    "These visualizations also helped identify specific failure modes, such as duplicate predictions of two call classes for the same ground-truth call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a6d90",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [WhisperSeg] Gu, N., Lee, K., Basha, M., Ram, S. K., You, G., & Hahnloser, R. H. (2024, April). Positive transfer of the whisper speech transformer to human and animal voice activity detection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7505-7509). IEEE.\n",
    "- [ActionFormer] Zhang, C. L., Wu, J., & Li, Y. (2022, October). Actionformer: Localizing moments of actions with transformers. In European Conference on Computer Vision (pp. 492-510). Cham: Springer Nature Switzerland.\n",
    "- [Whisper] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023, July). Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning (pp. 28492-28518). PMLR.\n",
    "- [NMS] Hosang, J., Benenson, R., & Schiele, B. (2017). Learning non-maximum suppression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4507-4515).\n",
    "- [CenterNet] Zhou, X., Wang, D., & Krahenbuhl, P. (2019). Objects as points. arXiv preprint arXiv:1904.07850.\n",
    "- [Macedonia] Macedonia, J. M. (1993). The vocal repertoire of the ring-tailed lemur (Lemur catta). Folia Primatologica, 61(4), 186-217.\n",
    "- [sigmoid] Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollar, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (pp. 2980-2988).\n",
    "- [DIoU] Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., & Ren, D. (2020, April). Distance-IoU loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 07, pp. 12993-13000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a77b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
