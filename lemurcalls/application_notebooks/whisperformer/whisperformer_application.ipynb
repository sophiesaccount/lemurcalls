{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to use the WhisperFormer Model - Step by Step Guide\n",
        "\n",
        "This notebook runs a trained WhisperFormer model on your audio files and saves the detected calls as `.json` files. Optionally, results can be converted to Raven selection tables.\n",
        "\n",
        "**Always run the cells in order from top to bottom!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install Dependencies\n",
        "\n",
        "You only need to run this cell the first time running the notebook (it might take some time).\n",
        "\n",
        "**After running this cell, restart the kernel** (Kernel → Restart Kernel) and then continue with Step 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install --user --upgrade torch torchvision numpy scipy transformers librosa pandas matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Import Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import logging\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from utils import infer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Set Paths and Parameters\n",
        "\n",
        "Before running: place your `.wav` files in the `audios/` folder next to this notebook.\n",
        "\n",
        "You also need:\n",
        "- **checkpoint file** (`.pth`) -- the trained WhisperFormer model\n",
        "- **whisper_config/** folder -- must contain `config.json` and `preprocessor_config.json` from the Whisper model used during training (copy from `whisper_models/whisper_base` or `whisper_models/whisper_large` in the main repository)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "PATH = os.getcwd()\n",
        "\n",
        "# --- Paths (adjust if needed) ---\n",
        "DATA_DIR = os.path.join(PATH, \"audios\")\n",
        "CHECKPOINT_PATH = os.path.join(PATH, \"checkpoint.pth\")\n",
        "WHISPER_CONFIG_PATH = os.path.join(PATH, \"whisper_config\")\n",
        "OUTPUT_DIR = os.path.join(PATH, \"jsons\")\n",
        "\n",
        "# --- Inference parameters ---\n",
        "THRESHOLD = 0.35        # minimum confidence score to keep a prediction\n",
        "IOU_THRESHOLD = 0.4     # IoU threshold for non-maximum suppression\n",
        "NUM_RUNS = 3            # number of offset runs (1 = fast, 3 = more robust)\n",
        "OVERLAP_TOLERANCE = 0.1 # IoU threshold for consolidating predictions across runs\n",
        "TOTAL_SPEC_COLUMNS = 3000\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Verify Paths\n",
        "\n",
        "Run this cell to check that all required paths exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_ok = True\n",
        "\n",
        "print(\"Checkpoint:\", CHECKPOINT_PATH)\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    size_mb = os.path.getsize(CHECKPOINT_PATH) / 1e6\n",
        "    print(f\"  OK ({size_mb:.0f} MB)\")\n",
        "else:\n",
        "    print(\"  MISSING -- please provide a .pth checkpoint file\")\n",
        "    all_ok = False\n",
        "\n",
        "print(\"Whisper config:\", WHISPER_CONFIG_PATH)\n",
        "if os.path.isdir(WHISPER_CONFIG_PATH):\n",
        "    contents = os.listdir(WHISPER_CONFIG_PATH)\n",
        "    print(f\"  OK (files: {contents})\")\n",
        "    for needed in [\"config.json\", \"preprocessor_config.json\"]:\n",
        "        if needed not in contents:\n",
        "            print(f\"  WARNING: {needed} is missing in whisper_config/\")\n",
        "            all_ok = False\n",
        "else:\n",
        "    print(\"  MISSING -- copy from whisper_models/whisper_base or whisper_large\")\n",
        "    all_ok = False\n",
        "\n",
        "print(\"Audio folder:\", DATA_DIR)\n",
        "if os.path.isdir(DATA_DIR):\n",
        "    wav_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".wav\")]\n",
        "    print(f\"  OK ({len(wav_files)} WAV file(s) found)\")\n",
        "    if not wav_files:\n",
        "        print(\"  WARNING: no .wav files found in audios/\")\n",
        "        all_ok = False\n",
        "else:\n",
        "    print(\"  MISSING -- create an 'audios' folder and place your .wav files there\")\n",
        "    all_ok = False\n",
        "\n",
        "print()\n",
        "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print()\n",
        "if all_ok:\n",
        "    print(\"Everything looks good! Proceed to Step 5.\")\n",
        "else:\n",
        "    print(\"Please fix the issues above before running inference.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Run Inference\n",
        "\n",
        "This will process all `.wav` files in `audios/` and save predictions as `.json` files in `jsons/`.\n",
        "\n",
        "The model runs each file multiple times with different time offsets and consolidates the results for more robust predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = infer(\n",
        "    data_dir=DATA_DIR,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    whisper_config_path=WHISPER_CONFIG_PATH,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    threshold=THRESHOLD,\n",
        "    iou_threshold=IOU_THRESHOLD,\n",
        "    total_spec_columns=TOTAL_SPEC_COLUMNS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_runs=NUM_RUNS,\n",
        "    overlap_tolerance=OVERLAP_TOLERANCE,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Results Summary\n",
        "\n",
        "Overview of the detected calls per file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "total = 0\n",
        "for filename, preds in results.items():\n",
        "    n = len(preds[\"onset\"])\n",
        "    total += n\n",
        "    clusters = {}\n",
        "    for c in preds[\"cluster\"]:\n",
        "        clusters[c] = clusters.get(c, 0) + 1\n",
        "    cluster_str = \", \".join(f\"{k}: {v}\" for k, v in sorted(clusters.items()))\n",
        "    print(f\"  {filename}: {n} predictions ({cluster_str})\")\n",
        "\n",
        "print(f\"\\nTotal: {total} predictions across {len(results)} file(s)\")\n",
        "print(f\"Results saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Visualize Predictions\n",
        "\n",
        "For each audio file, the first few segments are plotted showing:\n",
        "1. **Mel spectrogram** — the audio representation fed to the model\n",
        "2. **Per-class scores** — the raw model output (confidence per frame and class) with predicted call segments highlighted\n",
        "\n",
        "Adjust `NUM_SEGMENTS_TO_PLOT` to show more or fewer segments per file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import contextlib\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "\n",
        "from utils import load_trained_whisperformer, get_id_to_cluster\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "NUM_SEGMENTS_TO_PLOT = 3\n",
        "\n",
        "# --- Reload model & feature extractor for visualization ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, num_classes, detected_size = load_trained_whisperformer(\n",
        "    CHECKPOINT_PATH, device, WHISPER_CONFIG_PATH\n",
        ")\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
        "    WHISPER_CONFIG_PATH, local_files_only=True\n",
        ")\n",
        "\n",
        "id_to_cluster = get_id_to_cluster(num_classes)\n",
        "cluster_to_id = {v: k for k, v in id_to_cluster.items()}\n",
        "\n",
        "LABEL_DISPLAY = {\"m\": \"moan\", \"h\": \"hmm\", \"w\": \"wail\"}\n",
        "COLOR_MAP = {0: \"darkorange\", 1: \"cornflowerblue\", 2: \"gold\", 3: \"r\"}\n",
        "SEC_PER_COL = 0.02\n",
        "SR = 16000\n",
        "seg_duration = (TOTAL_SPEC_COLUMNS / 2) * SEC_PER_COL\n",
        "num_samples_in_clip = int(round((TOTAL_SPEC_COLUMNS * 0.01) * SR))\n",
        "\n",
        "use_autocast = device != \"cpu\" and (\n",
        "    (isinstance(device, str) and device.startswith(\"cuda\"))\n",
        "    or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
        ")\n",
        "autocast_ctx = (\n",
        "    torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
        "    if use_autocast else contextlib.nullcontext()\n",
        ")\n",
        "\n",
        "\n",
        "def plot_segment(mel_spec, class_scores, pred_onsets, pred_offsets,\n",
        "                 pred_classes, title=\"\", threshold=THRESHOLD):\n",
        "    \"\"\"Plot mel spectrogram with model scores and predicted segments.\"\"\"\n",
        "    T = class_scores.shape[0]\n",
        "    num_cls = class_scores.shape[1]\n",
        "    time_axis = np.arange(T) * SEC_PER_COL\n",
        "\n",
        "    fig, (ax_spec, ax_scores) = plt.subplots(\n",
        "        2, 1, figsize=(12, 5), height_ratios=[3, 1.2]\n",
        "    )\n",
        "\n",
        "    # Mel spectrogram\n",
        "    librosa.display.specshow(\n",
        "        mel_spec, cmap=plt.cm.magma, sr=SR, hop_length=160,\n",
        "        x_axis=\"time\", y_axis=\"mel\", fmin=0, fmax=8000, ax=ax_spec,\n",
        "    )\n",
        "    ax_spec.set_ylabel(\"Frequency (Hz)\")\n",
        "    ax_spec.set_title(title)\n",
        "\n",
        "    # Per-class score bars\n",
        "    for c in range(num_cls):\n",
        "        label = LABEL_DISPLAY.get(\n",
        "            id_to_cluster.get(c, \"\"), id_to_cluster.get(c, str(c))\n",
        "        )\n",
        "        ax_scores.bar(\n",
        "            time_axis, class_scores[:, c], width=SEC_PER_COL,\n",
        "            align=\"edge\", alpha=1, label=label,\n",
        "            color=COLOR_MAP.get(c, f\"C{c}\"),\n",
        "        )\n",
        "\n",
        "    ax_scores.axhline(\n",
        "        y=threshold, color=\"r\", linestyle=\"--\",\n",
        "        label=f\"Threshold {threshold}\",\n",
        "    )\n",
        "    ax_scores.set_ylim(0, 1.1)\n",
        "    ax_scores.set_xlim(0, T * SEC_PER_COL)\n",
        "    ax_scores.set_xlabel(\"Time (s)\")\n",
        "    ax_scores.set_ylabel(\"Score\")\n",
        "    ax_scores.set_title(\"WhisperFormer Scores and Predictions\")\n",
        "\n",
        "    # Highlight predicted segments\n",
        "    for onset, offset, cls in zip(pred_onsets, pred_offsets, pred_classes):\n",
        "        cid = cluster_to_id.get(cls, 0)\n",
        "        ax_scores.axvspan(\n",
        "            onset, offset, color=COLOR_MAP.get(cid, \"gray\"), alpha=0.3\n",
        "        )\n",
        "\n",
        "    # De-duplicated legend\n",
        "    handles, labels_leg = ax_scores.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels_leg, handles))\n",
        "    ax_scores.legend(by_label.values(), by_label.keys(), loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Loop over audio files and visualize ---\n",
        "wav_files = sorted(Path(DATA_DIR).rglob(\"*.[Ww][Aa][Vv]\"))\n",
        "\n",
        "for wav_path in wav_files:\n",
        "    preds = results.get(wav_path.name)\n",
        "    if preds is None:\n",
        "        continue\n",
        "\n",
        "    audio, _ = librosa.load(wav_path, sr=SR)\n",
        "    pred_onsets = np.array(preds[\"onset\"])\n",
        "    pred_offsets = np.array(preds[\"offset\"])\n",
        "    pred_clusters = np.array(preds[\"cluster\"])\n",
        "\n",
        "    for seg_i in range(NUM_SEGMENTS_TO_PLOT):\n",
        "        seg_start_sec = seg_i * seg_duration\n",
        "        seg_end_sec = (seg_i + 1) * seg_duration\n",
        "        seg_start_sample = int(seg_start_sec * SR)\n",
        "\n",
        "        if seg_start_sample >= len(audio):\n",
        "            break\n",
        "\n",
        "        clip = audio[seg_start_sample : seg_start_sample + num_samples_in_clip]\n",
        "        if len(clip) < SR * 0.1:\n",
        "            break\n",
        "\n",
        "        clip_padded = np.concatenate(\n",
        "            [clip, np.zeros(max(0, num_samples_in_clip - len(clip)))]\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        feats = feature_extractor(\n",
        "            clip_padded, sampling_rate=SR, padding=\"do_not_pad\"\n",
        "        )[\"input_features\"][0]\n",
        "        mel_spec = np.array(feats)\n",
        "\n",
        "        # Per-frame model scores\n",
        "        x = torch.tensor(feats, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        with torch.no_grad(), autocast_ctx:\n",
        "            class_preds, _ = model(x)\n",
        "            class_scores = torch.sigmoid(class_preds).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Select predictions falling within this segment\n",
        "        in_seg = (pred_onsets < seg_end_sec) & (pred_offsets > seg_start_sec)\n",
        "        seg_pred_onsets = np.clip(pred_onsets[in_seg] - seg_start_sec, 0, None)\n",
        "        seg_pred_offsets = np.clip(\n",
        "            pred_offsets[in_seg] - seg_start_sec, None, seg_duration\n",
        "        )\n",
        "        seg_pred_clusters = pred_clusters[in_seg]\n",
        "\n",
        "        plot_segment(\n",
        "            mel_spec, class_scores,\n",
        "            seg_pred_onsets, seg_pred_offsets, seg_pred_clusters,\n",
        "            title=f\"{wav_path.name} — Segment {seg_i + 1}\",\n",
        "            threshold=THRESHOLD,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### Step 8: Convert to Raven Selection Tables (Optional)\n",
        "\n",
        "To visualize the results in Raven, convert the `.json` files to `.txt` selection tables."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from json_to_raven import process_folder\n",
        "\n",
        "JSON_DIR = os.path.join(PATH, \"jsons\")\n",
        "RAVEN_DIR = os.path.join(PATH, \"raven\")\n",
        "\n",
        "process_folder(JSON_DIR, RAVEN_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `.txt` selection tables can now be found in the `raven/` folder. Open them in Raven Pro alongside the corresponding audio files to visualize the detected calls.\n",
        "\n",
        "---\n",
        "*End of notebook.*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}