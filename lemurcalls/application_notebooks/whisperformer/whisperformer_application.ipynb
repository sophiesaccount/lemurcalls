{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to use the WhisperFormer Model - Step by Step Guide\n",
        "\n",
        "This notebook runs a trained WhisperFormer model on your audio files and saves the detected calls as `.json` files. Optionally, results can be converted to Raven selection tables.\n",
        "\n",
        "**Always run the cells in order from top to bottom!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install Dependencies\n",
        "\n",
        "You only need to run this cell the first time running the notebook (it might take some time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install numpy scipy torch transformers librosa pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Import Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from utils import infer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Set Paths and Parameters\n",
        "\n",
        "Before running: place your `.wav` files in the `audios/` folder next to this notebook.\n",
        "\n",
        "You also need:\n",
        "- **checkpoint file** (`.pth`) -- the trained WhisperFormer model\n",
        "- **whisper_config/** folder -- must contain `config.json` and `preprocessor_config.json` from the Whisper model used during training (copy from `whisper_models/whisper_base` or `whisper_models/whisper_large` in the main repository)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "PATH = os.getcwd()\n",
        "\n",
        "# --- Paths (adjust if needed) ---\n",
        "DATA_DIR = os.path.join(PATH, \"audios\")\n",
        "CHECKPOINT_PATH = os.path.join(PATH, \"checkpoint.pth\")\n",
        "WHISPER_CONFIG_PATH = os.path.join(PATH, \"whisper_config\")\n",
        "OUTPUT_DIR = os.path.join(PATH, \"jsons\")\n",
        "\n",
        "# --- Inference parameters ---\n",
        "THRESHOLD = 0.35        # minimum confidence score to keep a prediction\n",
        "IOU_THRESHOLD = 0.4     # IoU threshold for non-maximum suppression\n",
        "NUM_RUNS = 3            # number of offset runs (1 = fast, 3 = more robust)\n",
        "OVERLAP_TOLERANCE = 0.1 # IoU threshold for consolidating predictions across runs\n",
        "TOTAL_SPEC_COLUMNS = 3000\n",
        "BATCH_SIZE = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Verify Paths\n",
        "\n",
        "Run this cell to check that all required paths exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_ok = True\n",
        "\n",
        "print(\"Checkpoint:\", CHECKPOINT_PATH)\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    size_mb = os.path.getsize(CHECKPOINT_PATH) / 1e6\n",
        "    print(f\"  OK ({size_mb:.0f} MB)\")\n",
        "else:\n",
        "    print(\"  MISSING -- please provide a .pth checkpoint file\")\n",
        "    all_ok = False\n",
        "\n",
        "print(\"Whisper config:\", WHISPER_CONFIG_PATH)\n",
        "if os.path.isdir(WHISPER_CONFIG_PATH):\n",
        "    contents = os.listdir(WHISPER_CONFIG_PATH)\n",
        "    print(f\"  OK (files: {contents})\")\n",
        "    for needed in [\"config.json\", \"preprocessor_config.json\"]:\n",
        "        if needed not in contents:\n",
        "            print(f\"  WARNING: {needed} is missing in whisper_config/\")\n",
        "            all_ok = False\n",
        "else:\n",
        "    print(\"  MISSING -- copy from whisper_models/whisper_base or whisper_large\")\n",
        "    all_ok = False\n",
        "\n",
        "print(\"Audio folder:\", DATA_DIR)\n",
        "if os.path.isdir(DATA_DIR):\n",
        "    wav_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".wav\")]\n",
        "    print(f\"  OK ({len(wav_files)} WAV file(s) found)\")\n",
        "    if not wav_files:\n",
        "        print(\"  WARNING: no .wav files found in audios/\")\n",
        "        all_ok = False\n",
        "else:\n",
        "    print(\"  MISSING -- create an 'audios' folder and place your .wav files there\")\n",
        "    all_ok = False\n",
        "\n",
        "print()\n",
        "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print()\n",
        "if all_ok:\n",
        "    print(\"Everything looks good! Proceed to Step 5.\")\n",
        "else:\n",
        "    print(\"Please fix the issues above before running inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Run Inference\n",
        "\n",
        "This will process all `.wav` files in `audios/` and save predictions as `.json` files in `jsons/`.\n",
        "\n",
        "The model runs each file multiple times with different time offsets and consolidates the results for more robust predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = infer(\n",
        "    data_dir=DATA_DIR,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    whisper_config_path=WHISPER_CONFIG_PATH,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    threshold=THRESHOLD,\n",
        "    iou_threshold=IOU_THRESHOLD,\n",
        "    total_spec_columns=TOTAL_SPEC_COLUMNS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_runs=NUM_RUNS,\n",
        "    overlap_tolerance=OVERLAP_TOLERANCE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Results Summary\n",
        "\n",
        "Overview of the detected calls per file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "total = 0\n",
        "for filename, preds in results.items():\n",
        "    n = len(preds[\"onset\"])\n",
        "    total += n\n",
        "    clusters = {}\n",
        "    for c in preds[\"cluster\"]:\n",
        "        clusters[c] = clusters.get(c, 0) + 1\n",
        "    cluster_str = \", \".join(f\"{k}: {v}\" for k, v in sorted(clusters.items()))\n",
        "    print(f\"  {filename}: {n} predictions ({cluster_str})\")\n",
        "\n",
        "print(f\"\\nTotal: {total} predictions across {len(results)} file(s)\")\n",
        "print(f\"Results saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 7: Convert to Raven Selection Tables (Optional)\n",
        "\n",
        "To visualize the results in Raven, convert the `.json` files to `.txt` selection tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from json_to_raven import process_folder\n",
        "\n",
        "JSON_DIR = os.path.join(PATH, \"jsons\")\n",
        "RAVEN_DIR = os.path.join(PATH, \"raven\")\n",
        "\n",
        "process_folder(JSON_DIR, RAVEN_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `.txt` selection tables can now be found in the `raven/` folder. Open them in Raven Pro alongside the corresponding audio files to visualize the detected calls."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
