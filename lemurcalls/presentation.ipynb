{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bbc26ba5",
      "metadata": {},
      "source": [
        "### Project Background\n",
        "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
        "\n",
        "The code for WhisperSeg—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942fee0d",
      "metadata": {},
      "source": [
        "### Intoduction\n",
        "\n",
        "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65179682",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9469d776",
      "metadata": {},
      "source": [
        "Biological Background and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298feaf3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "how to make the "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4a60a",
      "metadata": {},
      "source": [
        "Explaination of Models:\n",
        "\n",
        "WhisperSeg\n",
        "\n",
        "WhisperFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7313843e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "getting started:\n",
        "First, run the below code to install lemurcalls as editable package with the dependancies from pyproject.toml."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb118193",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pip install -e "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d749893",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "72d06596",
      "metadata": {},
      "source": [
        "how to run WhisperSeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5daae753",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "how to run WhisperFormer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8137b8a",
      "metadata": {},
      "source": [
        "## Thresholds and visualization\n",
        "\n",
        "If the aim is to only detect high-quality calls from the focal animal, it can be usefull to apply postprocessing filters, such as SNR and amplitude filters.\n",
        "SNR = "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa592e12",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "To determine the appropriate thresholds for your dataset, you can plot the SNR and maximale amplitudes of the calls in your testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967f77df",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score\n",
        " --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/best_models_new/final_model_20251205_030535/best_model.pth \n",
        "--audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \n",
        "--label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \n",
        "--output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/best_models/final_model_20251205_030535/snr_amplitude_plot \n",
        "--threshold 0.6 \n",
        "--iou_threshold 0.4 \n",
        "--num_classes 3 \n",
        "--batch_size 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a865e906",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Visualization of model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50b759",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
