{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b3c8893",
      "metadata": {},
      "source": [
        "## Documentation of 'Lemurcalls'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87bfbd32",
      "metadata": {},
      "source": [
        "### Project Background\n",
        "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
        "\n",
        "The code for WhisperSeg—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming.\n",
        "\n",
        "For more details on the project see `thesis.pdf`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744b526b",
      "metadata": {},
      "source": [
        "### Intoduction\n",
        "\n",
        "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. \n",
        "\n",
        "With this code, you can train two types of models that both detect, segment and classify lemur calls from long audio recordings. The libary also enables you to assess the performance of the models.\n",
        "To further imporve results,  the model predictions can be filtered by applying SNR and maximal amplitude thresholds. To determine those thresholds, the SNR and amplitude values for the calls in the training set can be plotted. To analyse model performance, the points can be colored by the predicted confidence scores. \n",
        "Further, you can analyse and compare the model predcitions on the test set, by plotting the spectrograms as calculated by Whisper, the final predicted calls and, for WhisperFormer, the frame-wise confidence scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141a8402",
      "metadata": {},
      "source": [
        "### Library Structure:\n",
        "\n",
        "lemurcalls is organized into two main model subpackages, lemurcalls.whisperseg and lemurcalls.whisperformer, each covering training, inference, and evaluation workflows.\n",
        "Shared data handling and utility logic is centralized in common helper modules so both pipelines use consistent preprocessing and label handling.\n",
        "In addition, the library provides visualization and postprocessing tools (e.g., SNR/amplitude filtering and precision-recall analysis) to systematically inspect and improve model outputs. A more detailed visualization of the library structure can be seen below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc38999b",
      "metadata": {},
      "source": [
        "```text\n",
        "lemurcalls/\n",
        "├── README.md\n",
        "├── pyproject.toml\n",
        "├── presentation_notebook/\n",
        "│   └── presentation.ipynb\n",
        "└── lemurcalls/\n",
        "    ├── __init__.py\n",
        "    ├── datautils.py\n",
        "    ├── audio_utils.py\n",
        "    ├── utils.py\n",
        "    ├── visualize_predictions.py\n",
        "    ├── download_whipser.py\n",
        "    ├── whisperseg/\n",
        "    │   ├── __init__.py\n",
        "    │   ├── model.py\n",
        "    │   ├── train.py\n",
        "    │   ├── infer.py\n",
        "    │   ├── infer_folder.py\n",
        "    │   ├── evaluate.py\n",
        "    │   ├── evaluate_metrics.py\n",
        "    │   ├── training_utils.py\n",
        "    │   ├── datautils_ben.py\n",
        "    │   ├── utils.py\n",
        "    │   └── convert_hf_to_ct2.py\n",
        "    └── whisperformer/\n",
        "        ├── __init__.py\n",
        "        ├── model.py\n",
        "        ├── dataset.py\n",
        "        ├── datautils.py\n",
        "        ├── losses.py\n",
        "        ├── train.py\n",
        "        ├── infer.py\n",
        "        ├── postprocessing/\n",
        "        │   ├── prec_rec.py\n",
        "        │   └── filter_labels_by_snr.py\n",
        "        └── visualization/\n",
        "            └── scatterplot_ampl_snr_score.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65179682",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "298feaf3",
      "metadata": {},
      "source": [
        "### Biological and Mathematical Background\n",
        "\n",
        "#### Datasets:\n",
        "The data are audio recordings in the form of .wav files collected at the Affenwald STraußpark in Thüringen. To obtain the data, ring-tailed lemurs were equipped with collors equipped with microphones.\n",
        "The data for training was manually annotated with the Raven Pro Software: For each detected call onset and offset aswell as a class label were assigned. Only calls belonging to three specific types of calls ('moan', 'wail' and 'hmm') were labeled.\n",
        "\n",
        "The recordings are affected by diverse background noise (e.g. visitors of the park, traffic from a nearby road,...) and by the fact that microphones worn by one individual often capture calls from nearby conspecifics. To account for this variation, each call was manually classified into one of three quality classes:\n",
        "1. Quality 1: Loud, high-quality calls that almost certainly originate from the focal individual.\n",
        "2. Quality 2: Calls of moderate quality that probably do not originate from the focal individual.\n",
        "3. Quality 3: Low-quality background calls, including very quiet or distant vocalizations.\n",
        "\n",
        "The annotated JSON files are expanded to include quality labels and have the following structure:\n",
        "`{onset:[], offset:[], cluster:[], quality:[]}`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d31d27",
      "metadata": {},
      "source": [
        "## Explaination of the Models:\n",
        "\n",
        "WhisperSeg: [WhisperSeg] by Gu et al. leverages the pretrained [Whisper] Transformer—an automatic speech recognition system pretrained on 680,000 hours of multilingual and multitask supervised human speech. They showed that it can be effectively adapted to detect and classify animal vocalizations across multiple species and built a multi-species checkpoint. As a sequence-to sequence model, WhisperSeg outputs for each call onset, offset and call class.\n",
        "\n",
        "WhisperFormer: Instead of performing next-token-prediction as in WhisperSeg, WhisperFormer directly predicts the call centers and regresses on- and offsets. This center-based approach is inspired by and TAL methods as CenterNet and ActionFormer. To achieve this, WhisperFormer combines the original Whisper Encoder with a lightweight Decoder followed by a Regression Head and Classification Head. These Heads are inspired by [ActionFormer]. The training loss consits of two parts, directly linked of the two model heads. As in~\\cite{zhang2022actionformer} I use sigmoid focal loss~\\cite{lin2017focal} as class loss $\\mathcal{L}_{class}$ and dIoU loss~\\cite{zheng2020distance} as regression loss $\\mathcal{L}_{reg}$.\n",
        "The total loss can be described as the weighted sum of class and regression loss multiplied with the total number of positive samples. The regression loss only gets evaluated for samples $t$ with positive ground truth value. Let $CL$ be the ground truth clusters, $\\tilde{CL}$ the predicted clusters, $S$ the ground truth segments and $\\tilde{S}$ the predicted segments. Then the total loss can be described as\n",
        "$$ \\mathcal{L}_{total}(CL, S, \\tilde{CL}, \\tilde{S})  = \\frac{1}{T_{+}} \\left( \\mathcal{L}_{class}(CL, \\tilde{CL})  + \\lambda \\cdot \\mathbb{1}_{\\mathcal{T}_+} \\cdot \\mathcal{L}_{reg}(S, \\tilde{S})\\right).$$\n",
        "\n",
        "WhisperFormer outputs for each spectrogram column and each of the three classes a confidence score for there to be a and relative on and offsets given a call with center at the given spectrogram column. \n",
        "Thus, for the final predictions non-maximum suppression ([NMS]) is performed over all calls and a confidence threshold are applied. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4a60a",
      "metadata": {},
      "source": [
        "\n",
        "#### Evaluation Metrics:\n",
        "\n",
        "In line with standard practice in detection tasks, I use the F1 score as the primary evaluation metric for all calls. Let $TP$ denote the number of true positives, $FP$ the number of false positives, $FN$ the number of false negatives, and $FC$ the number of predictions that match the temporal location of a call but are assigned an incorrect class. Then, precision and recall are defined as:\n",
        "$$precision = \\frac{TP}{TP + FP + FC}$$\n",
        "and \n",
        "$$recall = \\frac{TP}{TP+FN+FC}.$$\n",
        "Precision measures the accuracy of the predicted calls, while recall quantifies the proportion of ground truth calls correctly detected. The F1 score is the harmonic mean of precision and recall, balancing these two aspects:\n",
        "$$F1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}.$$\n",
        "Following standard practice in multiclass classification, a FC prediction contributes both a FP (for the incorrectly predicted class) and a FN (for the true class). \n",
        "\n",
        "##### Evaluation by Quality Classes\n",
        "To account for differences in call quality, we calculate F1 scores with respect to the ground truth quality classes. I distinguish the following metrics:\n",
        "1.  $F1_{Q1,Q2,Q3}$: F1 score calculated with respect to the ground truth labels of all quality classes 1,2 and 3. \n",
        "2. $F1_{Q1,Q2}$: F1 score calculated with respect to the ground truth labels of calls from quality classes 1 and 2. \n",
        "3. $F1_{Q1}$: F1 score calculated with respect to the ground truth labels of quality classes 1 only.\n",
        "\n",
        "Formally, for any subset of quality classes, the F1 score is computed as in ~\\ref{f1}, with precision and recall restricted to the selected quality classes.\n",
        "\n",
        "A limitation of this approach is that, when focusing on detecting high-quality calls (quality class 1), false positives from lower-quality classes (Q2 or Q3) may be less bad than completely missing detections. To address this, I define adjusted F1 metrics:\n",
        "1. $F1_{Q1,(Q1,Q3)}$: F1 score calculated with respect to the ground truth labels of quality class 1, but FP from q2 and q3 are counted as neither TPs nor FPs.\n",
        "2. $F1_{Q1,(Q2,Q3)}$: F1 score calculated with respect to the ground truth labels of quality classes 1 and 2, but FP from q3 are counted as neither TPs nor FPs.\n",
        "\n",
        "In these adjusted metrics, $TP$, $FN$, and $FC$ remain unchanged compared to $F1_{Q1}$ or $F1_{Q1,Q2}$, but the number of false positives may decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7313843e",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "First, run the below code to install lemurcalls as editable package with the dependancies from `pyproject.toml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb118193",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -e . "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fc27e7",
      "metadata": {},
      "source": [
        "The library includes to subpackages lemurcalls.whisperseg and lemurcalls.whisperformer as well as a tool to visualize and compare the predictions achieved with the trained models.\n",
        "In the following we set some demo paths, to demonstrate the functionalities of the python libraries. Since for training large models, you need a GPU, for demonstartion purposes I used small maximal numbers of epochs and a single audio file for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d749893",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = \"/projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls\"\n",
        "AUDIO_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_test\"\n",
        "AUDIO_SINGLE_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_single\"\n",
        "LABEL_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/jsons_test\"\n",
        "\n",
        "WHISPER_BASE_PATH = f\"{PROJECT_ROOT}/whisper_models/whisper_base\"\n",
        "WHISPERSEG_TRAIN_OUT = f\"{PROJECT_ROOT}/lemurcalls/whisperseg_models\"\n",
        "WHISPERSEG_MODEL_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\"\n",
        "\n",
        "WHISPERFORMER_CKPT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth\"\n",
        "WHISPERFORMER_PRED_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-20_11-48-55\"\n",
        "WHISPERFORMER_VIS_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/visualization\"\n",
        "WHISPERFORMER_SNR_OUT = \"/mnt/lustre-grete/usr/u17327/final/jsons_test_filtered\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d06596",
      "metadata": {},
      "source": [
        "## The WhisperSeg Subpackage\n",
        "\n",
        "To train a whisperseg model, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0878c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Using fixed codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
            "Created 609 training samples after slicing\n",
            "epoch-000:   0%|                                        | 0/152 [00:00<?, ?it/s]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "epoch-000:  26%|████████▏                      | 40/152 [03:37<10:05,  5.41s/it]^C\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.train \\\n",
        "  --initial_model_path \"{WHISPER_BASE_PATH}\" \\\n",
        "  --model_folder \"{WHISPERSEG_TRAIN_OUT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --n_threads 1 \\\n",
        "  --num_workers 1 \\\n",
        "  --max_num_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ceb55b",
      "metadata": {},
      "source": [
        "For inference, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f8bd3bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Found 1 wav files in: /mnt/lustre-grete/usr/u17327/final/audios_single\n",
            "INFO:root:Current file: [  0] /mnt/lustre-grete/usr/u17327/final/audios_single/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.WAV\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.infer \\\n",
        "  -d \"{AUDIO_SINGLE_DIR}\" \\\n",
        "  -m \"{WHISPERSEG_MODEL_DIR}\" \\\n",
        "  -o \"{WHISPERSEG_MODEL_DIR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc52565",
      "metadata": {},
      "source": [
        "For evaluation, you can use the original WhisperSeg evaluation.py or use evaluate_metrics.py (that is also applicable for WhisperFormer), that can be run via"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b5101c",
      "metadata": {},
      "outputs": [],
      "source": [
        "python -m lemurcalls.whisperseg.evaluate_metrics \\\n",
        "  --labels \"/mnt/lustre-grete/usr/u17327/final/jsons_test/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.json\" \\\n",
        "  --predictions \"/path/to/your_prediction_file.json\" \\\n",
        "  --overlap_tolerance 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5daae753",
      "metadata": {},
      "source": [
        "### The WhisperFormer Subpackage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46b1abb",
      "metadata": {},
      "source": [
        "To train a WhisperFormer model, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9484e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.train \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --model_folder /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --max_num_epochs 1 \\\n",
        "  --whisper_size large"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bd5395",
      "metadata": {},
      "source": [
        "For inference with a set confidence score threshold, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acc6f70",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.infer \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_single \\\n",
        "  --output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/sc \\\n",
        "  --batch_size 4 \\\n",
        "  --iou_threshold 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0d0c6c",
      "metadata": {},
      "source": [
        "To identify a suitable confidence threshold and assess overall model behavior, you can compute a precision-recall curve across multiple score thresholds.  \n",
        "This helps you choose a threshold that best matches your objective (e.g., higher precision to reduce false positives, or higher recall to miss fewer calls).\n",
        "\n",
        "You can also control which label quality classes are considered during evaluation via `eval_mode`:\n",
        "\n",
        "- `standard`: evaluates with respect to the $F_1$, $F_{1,2}$ or $F_{1,2,3}$.\n",
        "- `q3_q2`: applies the quality-aware evaluation strategy, where $F_{1,2,3}$ is calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4fbcab",
      "metadata": {},
      "outputs": [],
      "source": [
        "WHISPERFORMER_PREC_REC_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/prec_rec\"\n",
        "\n",
        "!python -m lemurcalls.whisperformer.postprocessing.prec_rec \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_PREC_REC_OUT}\" \\\n",
        "  --overlap_tolerance 0.3 \\\n",
        "  --allowed_qualities 1 2 3 \\\n",
        "  --eval_mode q3_q2 \\\n",
        "  --thresholds 0.1 0.15 0.2 0.25 0.3 0.35"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8137b8a",
      "metadata": {},
      "source": [
        "## Thresholds and visualization\n",
        "\n",
        "If the aim is to only detect high-quality calls from the focal animal, it can be usefull to apply postprocessing filters, such as Signal-to_Noise Ratio (SNR) and amplitude filters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa592e12",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "To determine the appropriate thresholds for your dataset, you can plot the SNR and maximale amplitudes of the calls in your training set and color them by quality class. Additionally you can color the calls according to the confidence score assigned by a final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967f77df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-27_11-17-53/run_arguments.json\n",
            "Checkpoint: whisper_size=large, num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "^C                               \n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed3ff16",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Quality)](scatter_snr_vs_amplitude.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b49095",
      "metadata": {},
      "source": [
        "We can see in ourdataset when we plot SNR and maximal amplitude of the labeled calls, that quality class 2 and quality class 3 cannot be (linearly) seperated easily. Quality class 1 on the other hand can - with the chosen thresholds - be seperated rather well from the other quality classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6dc3c9",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Model Score)](scatter_snr_vs_amplitude_model_score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec937209",
      "metadata": {},
      "source": [
        "To filter existing predictions by SNR and maximal amplitude use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b996781",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.postprocessing.filter_labels_by_snr \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_SNR_OUT}\" \\\n",
        "  --snr_threshold -1 \\\n",
        "  --amplitude_threshold 0.035"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e866d0f",
      "metadata": {},
      "source": [
        "## Visualization of model outputs\n",
        "To visualize, analyze and compare the predictions of two trained models, you can use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a865e906",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-26_22-51-18/run_arguments.json\n",
            "Checkpoint: num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "Detected Whisper size: large\n",
            "Model loaded (Whisper large)\n",
            "                                 \r"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.visualize_predictions \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_label_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a62faf1",
      "metadata": {},
      "source": [
        "The audio recordings are split into parts of 30 seconds duration and for each part an output with the following four rows is created:\n",
        "1. The spectrograms as calculated by the original Whisper Encoder\n",
        "2. For each spectrogram column and each call class the confidence score outputted by the WhisperFormer model are shown. The final predictions above the specified threshold (indicated as red dotted line) are \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2175091d",
      "metadata": {},
      "source": [
        "![Spec1](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_00_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "604cb979",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d034d135",
      "metadata": {},
      "outputs": [],
      "source": [
        "![Spec2](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_01_spectrogram_scores_gt copy.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f700c6a",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692cf4d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "![Spec3](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_01_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa1809",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02aa952",
      "metadata": {},
      "outputs": [],
      "source": [
        "![Spec4](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_02_spectrogram_scores_gt copy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0269fb",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d15a4b53",
      "metadata": {},
      "source": [
        "![Spec5](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_02_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561f88fe",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fd50b759",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16754d83",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "09e0832d",
      "metadata": {},
      "source": [
        "### Refernces\n",
        "\n",
        "[ActionFormer] "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4a6d90",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
