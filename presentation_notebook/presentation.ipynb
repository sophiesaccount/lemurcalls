{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b3c8893",
      "metadata": {},
      "source": [
        "## Documentation of 'Lemurcalls'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744b526b",
      "metadata": {},
      "source": [
        "### Intoduction\n",
        "\n",
        "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. \n",
        "\n",
        "With this code, you can train two types of models that both detect, segment and classify lemur calls from long audio recordings. The libary also enables you to assess the performance of the models.\n",
        "To further imporve results,  the model predictions can be filtered by applying SNR and maximal amplitude thresholds. To determine those thresholds, the SNR and amplitude values for the calls in the training set can be plotted. To analyse model performance, the points can be colored by the predicted confidence scores. \n",
        "Further, you can analyse and compare the model predcitions on the test set, by plotting the spectrograms as calculated by Whisper, the final predicted calls and, for WhisperFormer, the frame-wise confidence scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141a8402",
      "metadata": {},
      "source": [
        "### Library Structure:\n",
        "\n",
        "lemurcalls is organized into two main model subpackages, lemurcalls.whisperseg and lemurcalls.whisperformer, each covering training, inference, and evaluation workflows.\n",
        "Shared data handling and utility logic is centralized in common helper modules so both pipelines use consistent preprocessing and label handling.\n",
        "In addition, the library provides visualization and postprocessing tools (e.g., SNR/amplitude filtering and precision-recall analysis) to systematically inspect and improve model outputs. A more detailed visualization of the library structure can be seen below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc38999b",
      "metadata": {},
      "source": [
        "```text\n",
        "lemurcalls/\n",
        "├── README.md\n",
        "├── pyproject.toml\n",
        "├── presentation_notebook/\n",
        "│   └── presentation.ipynb\n",
        "└── lemurcalls/\n",
        "    ├── __init__.py\n",
        "    ├── datautils.py\n",
        "    ├── audio_utils.py\n",
        "    ├── utils.py\n",
        "    ├── visualize_predictions.py\n",
        "    ├── download_whipser.py\n",
        "    ├── whisperseg/\n",
        "    │   ├── __init__.py\n",
        "    │   ├── model.py\n",
        "    │   ├── train.py\n",
        "    │   ├── infer.py\n",
        "    │   ├── infer_folder.py\n",
        "    │   ├── evaluate.py\n",
        "    │   ├── evaluate_metrics.py\n",
        "    │   ├── training_utils.py\n",
        "    │   ├── datautils_ben.py\n",
        "    │   ├── utils.py\n",
        "    │   └── convert_hf_to_ct2.py\n",
        "    └── whisperformer/\n",
        "        ├── __init__.py\n",
        "        ├── model.py\n",
        "        ├── dataset.py\n",
        "        ├── datautils.py\n",
        "        ├── losses.py\n",
        "        ├── train.py\n",
        "        ├── infer.py\n",
        "        ├── postprocessing/\n",
        "        │   ├── prec_rec.py\n",
        "        │   └── filter_labels_by_snr.py\n",
        "        └── visualization/\n",
        "            └── scatterplot_ampl_snr_score.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65179682",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "298feaf3",
      "metadata": {},
      "source": [
        "#### Datasets:\n",
        "The data are audio recordings in teh form of .wav files collected at the Affenwald STraußpark in Thüringen. To obtain the data, ring-tailed lemurs were equipped with collors equipped with microphones.\n",
        "The data for training was manually annotated with the Raven Pro Software: For each detected call onset and offset aswell as a class label were assigned. Only calls belonging to three specific types of calls ('moan', 'wail' and 'hmm') were labeled.\n",
        "\n",
        "The recordings are affected by diverse background noise (e.g. visitors of the park, traffic from a nearby road,...) and by the fact that microphones worn by one individual often capture calls from nearby conspecifics. To account for this variation, each call was manually classified into one of three quality classes:\n",
        "1. Quality 1: Loud, high-quality calls that almost certainly originate from the focal individual.\n",
        "2. Quality 2: Calls of moderate quality that probably do not originate from the focal individual.\n",
        "3. Quality 3: Low-quality background calls, including very quiet or distant vocalizations.\n",
        "\n",
        "The annotated JSON files are expanded to include quality labels and have the following structure:\n",
        "`{onset:[], offset:[], cluster:[], quality:[]}`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4a60a",
      "metadata": {},
      "source": [
        "## Explaination of the Models:\n",
        "\n",
        "WhisperSeg:\n",
        "\n",
        "WhisperFormer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7313843e",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "First, run the below code to install lemurcalls as editable package with the dependancies from `pyproject.toml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb118193",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -e . "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fc27e7",
      "metadata": {},
      "source": [
        "The library includes to subpackages lemurcalls.whisperseg and lemurcalls.whisperformer as well as a tool to visualize and compare the predictions achieved with the trained models.\n",
        "In the following we set some demo paths, to demonstrate the functionalities of the python libraries. Since for training large models, you need a GPU, for demonstartion purposes I used small maximal numbers of epochs and a single audio file for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d749893",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = \"/projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls\"\n",
        "AUDIO_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_test\"\n",
        "AUDIO_SINGLE_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_single\"\n",
        "LABEL_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/jsons_test\"\n",
        "\n",
        "WHISPER_BASE_PATH = f\"{PROJECT_ROOT}/whisper_models/whisper_base\"\n",
        "WHISPERSEG_TRAIN_OUT = f\"{PROJECT_ROOT}/lemurcalls/whisperseg_models\"\n",
        "WHISPERSEG_MODEL_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\"\n",
        "\n",
        "WHISPERFORMER_CKPT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth\"\n",
        "WHISPERFORMER_PRED_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-20_11-48-55\"\n",
        "WHISPERFORMER_VIS_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/visualization\"\n",
        "WHISPERFORMER_SNR_OUT = \"/mnt/lustre-grete/usr/u17327/final/jsons_test_filtered\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d06596",
      "metadata": {},
      "source": [
        "## The WhisperSeg Subpackage\n",
        "\n",
        "To train a whisperseg model, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0878c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Using fixed codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
            "Created 609 training samples after slicing\n",
            "epoch-000:   0%|                                        | 0/152 [00:00<?, ?it/s]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "epoch-000:  26%|████████▏                      | 40/152 [03:37<10:05,  5.41s/it]^C\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.train \\\n",
        "  --initial_model_path \"{WHISPER_BASE_PATH}\" \\\n",
        "  --model_folder \"{WHISPERSEG_TRAIN_OUT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --n_threads 1 \\\n",
        "  --num_workers 1 \\\n",
        "  --max_num_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ceb55b",
      "metadata": {},
      "source": [
        "For inference, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f8bd3bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Found 1 wav files in: /mnt/lustre-grete/usr/u17327/final/audios_single\n",
            "INFO:root:Current file: [  0] /mnt/lustre-grete/usr/u17327/final/audios_single/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.WAV\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.infer \\\n",
        "  -d \"{AUDIO_SINGLE_DIR}\" \\\n",
        "  -m \"{WHISPERSEG_MODEL_DIR}\" \\\n",
        "  -o \"{WHISPERSEG_MODEL_DIR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc52565",
      "metadata": {},
      "source": [
        "For evaluation, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b5101c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5daae753",
      "metadata": {},
      "source": [
        "### The WhisperFormer Subpackage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46b1abb",
      "metadata": {},
      "source": [
        "To train a WhisperFormer model, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9484e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.train \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --model_folder /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --max_num_epochs 1 \\\n",
        "  --whisper_size large"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bd5395",
      "metadata": {},
      "source": [
        "For inference with a set confidence score threshold, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acc6f70",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.infer \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_single \\\n",
        "  --output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/sc \\\n",
        "  --batch_size 4 \\\n",
        "  --iou_threshold 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0d0c6c",
      "metadata": {},
      "source": [
        "To identify a suitable confidence threshold and assess overall model behavior, you can compute a precision-recall curve across multiple score thresholds.  \n",
        "This helps you choose a threshold that best matches your objective (e.g., higher precision to reduce false positives, or higher recall to miss fewer calls).\n",
        "\n",
        "You can also control which label quality classes are considered during evaluation via `eval_mode`:\n",
        "\n",
        "- `standard`: evaluates with the default quality handling.\n",
        "- `q3_q2`: applies the quality-aware evaluation strategy where quality classes 2 and 3 are treated differently from class 1.\n",
        "\n",
        "Choose the mode based on your analysis goal.  \n",
        "For example, if you mainly care about high-quality focal calls, use `q3_q2`; for a more general benchmark, use `standard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4fbcab",
      "metadata": {},
      "outputs": [],
      "source": [
        "WHISPERFORMER_PREC_REC_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/prec_rec\"\n",
        "\n",
        "!python -m lemurcalls.whisperformer.postprocessing.prec_rec \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_PREC_REC_OUT}\" \\\n",
        "  --overlap_tolerance 0.3 \\\n",
        "  --allowed_qualities 1 2 3 \\\n",
        "  --eval_mode q3_q2 \\\n",
        "  --thresholds 0.1 0.15 0.2 0.25 0.3 0.35"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8137b8a",
      "metadata": {},
      "source": [
        "## Thresholds and visualization\n",
        "\n",
        "If the aim is to only detect high-quality calls from the focal animal, it can be usefull to apply postprocessing filters, such as SNR and amplitude filters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa592e12",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "To determine the appropriate thresholds for your dataset, you can plot the SNR and maximale amplitudes of the calls in your testset and color them by quality class. Additionally you can color the calls according to the confidence score assigned by a final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967f77df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-27_11-17-53/run_arguments.json\n",
            "Checkpoint: whisper_size=large, num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "^C                               \n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed3ff16",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Quality)](scatter_snr_vs_amplitude.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b49095",
      "metadata": {},
      "source": [
        "We can see in ourdataset when we plot SNR and maximal amplitude of the labeled calls, that quality class 2 and quality class 3 cannot be linearly seperated easily. Qulaity class 1 on teh other ahdn can with the chosen thresholds be seperated rather well from the other quality classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6dc3c9",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Model Score)](scatter_snr_vs_amplitude_model_score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec937209",
      "metadata": {},
      "source": [
        "To filter existing outputs use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b996781",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.postprocessing.filter_labels_by_snr \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_SNR_OUT}\" \\\n",
        "  --snr_threshold -1 \\\n",
        "  --amplitude_threshold 0.035"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e866d0f",
      "metadata": {},
      "source": [
        "## Visualization of model outputs\n",
        "To visualize the output of the model, you can use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a865e906",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-26_22-51-18/run_arguments.json\n",
            "Checkpoint: num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "Detected Whisper size: large\n",
            "Model loaded (Whisper large)\n",
            "                                 \r"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.visualize_predictions \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_label_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561f88fe",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fd50b759",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16754d83",
      "metadata": {},
      "source": [
        "### Project Background\n",
        "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
        "\n",
        "The code for WhisperSeg—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
