{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b3c8893",
      "metadata": {},
      "source": [
        "## Documentation of 'Lemurcalls'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87bfbd32",
      "metadata": {},
      "source": [
        "### Project Background\n",
        "This code is part of my master's thesis, *Center-Based Segmentation of Lemur Vocalizations Using the Whisper Audio Foundation Model* (submitted on 08.12.2025), but it was not part of the formal evaluation of this thesis or any other university module.\n",
        "\n",
        "The code for [WhisperSeg]—including `util`, `utils.py`, `datautils.py`, and `audio_utils.py`—was adapted from https://github.com/nianlonggu/WhisperSeg for the new dataset. For WhisperFormer, `losses.py` was adapted from https://github.com/happyharrycn/actionformer_release. I used ChatGPT and Cursor for debugging, code and text drafting, and brainstorming.\n",
        "\n",
        "For more details on the project see `thesis.pdf`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744b526b",
      "metadata": {},
      "source": [
        "### Intoduction\n",
        "\n",
        "Consistently detecting, segmenting, and classifying animal vocalizations is crucial for the study and conservation of wildlife. Manual annotation, however, is time-consuming and labor-intensive, highlighting the need for reliable automated approaches. Deep learning methods—especially those leveraging transfer learning—have achieved promising results in Sound Event Detection in recent years. Yet, performance often declines when models are applied to small datasets with diverse and high background noise, as is typical for primate vocalizations. \n",
        "\n",
        "With this code, you can train two types of models that both detect, segment and classify lemur calls from long audio recordings. The libary also enables you to assess the performance of the models.\n",
        "To further imporve results,  the model predictions can be filtered by applying SNR and maximal amplitude thresholds. To determine those thresholds, the SNR and amplitude values for the calls in the training set can be plotted. To analyse model performance, the points can be colored by the predicted confidence scores. \n",
        "Further, you can analyse and compare the model predcitions on the test set, by plotting the spectrograms as calculated by Whisper, the final predicted calls and, for WhisperFormer, the frame-wise confidence scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141a8402",
      "metadata": {},
      "source": [
        "### Library Structure:\n",
        "\n",
        "lemurcalls is organized into two main model subpackages, lemurcalls.whisperseg and lemurcalls.whisperformer, each covering training, inference, and evaluation workflows.\n",
        "Shared data handling and utility logic is centralized in common helper modules so both pipelines use consistent preprocessing and label handling.\n",
        "In addition, the library provides visualization and postprocessing tools (e.g., SNR/amplitude filtering and precision-recall analysis) to systematically inspect and improve model outputs. The project also includes two Jupyter notebooks for easy, web-application-friendly usage (e.g., for biology bachelor students).  \n",
        "A more detailed overview of the library structure is shown below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc38999b",
      "metadata": {},
      "source": [
        "```text\n",
        "lemurcalls/\n",
        "├── README.md\n",
        "├── pyproject.toml\n",
        "├── presentation_notebook/\n",
        "│   └── presentation.ipynb\n",
        "└── lemurcalls/\n",
        "    ├── __init__.py\n",
        "    ├── datautils.py\n",
        "    ├── audio_utils.py\n",
        "    ├── utils.py\n",
        "    ├── visualize_predictions.py\n",
        "    ├── download_whipser.py\n",
        "    ├── whisperseg/\n",
        "    │   ├── __init__.py\n",
        "    │   ├── model.py\n",
        "    │   ├── train.py\n",
        "    │   ├── infer.py\n",
        "    │   ├── infer_folder.py\n",
        "    │   ├── evaluate.py\n",
        "    │   ├── evaluate_metrics.py\n",
        "    │   ├── training_utils.py\n",
        "    │   ├── datautils_ben.py\n",
        "    │   ├── utils.py\n",
        "    │   └── convert_hf_to_ct2.py\n",
        "    └── whisperformer/\n",
        "        ├── __init__.py\n",
        "        ├── model.py\n",
        "        ├── dataset.py\n",
        "        ├── datautils.py\n",
        "        ├── losses.py\n",
        "        ├── train.py\n",
        "        ├── infer.py\n",
        "        ├── postprocessing/\n",
        "        │   ├── prec_rec.py\n",
        "        │   └── filter_labels_by_snr.py\n",
        "        └── visualization/\n",
        "            └── scatterplot_ampl_snr_score.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee04d4eb",
      "metadata": {},
      "source": [
        "#### Datasets\n",
        "\n",
        "The dataset consists of `.wav` audio recordings collected at the Affenwald Strausberg Park (Thuringia, Germany) with a total length of 4.8 hours. Each audio was resampled to 16 kHz. \n",
        "For data acquisition, ring-tailed lemurs were equipped with collars containing microphones.\n",
        "\n",
        "Training data were manually annotated in Raven Pro. For each detected call, annotators assigned onset time, offset time, and a class label.  \n",
        "Only three call types similar to the target call `moan` (see [Macedonia]) were included: `moan`, `wail`, and `hmm`.\n",
        "\n",
        "The recordings contain substantial background noise (e.g., visitor voices, nearby road traffic). In addition, microphones worn by one individual often capture calls from nearby conspecifics.  \n",
        "To account for this variability, each labeled call was assigned one of three quality classes:\n",
        "\n",
        "1. **Quality 1**: loud, high-quality calls that very likely originate from the focal individual.  \n",
        "2. **Quality 2**: medium-quality calls that likely originate from non-focal individuals.  \n",
        "3. **Quality 3**: low-quality background calls, including very quiet or distant vocalizations.\n",
        "\n",
        "Annotated labels are stored as `.json` files with the structure:  \n",
        "`{ \"onset\": [], \"offset\": [], \"cluster\": [], \"quality\": [] }`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ffdffb",
      "metadata": {},
      "source": [
        "## Explanation of the Models\n",
        "\n",
        "#### WhisperSeg\n",
        "\n",
        "[WhisperSeg] (Gu et al.) builds on the pretrained [Whisper] Transformer, an automatic speech recognition model trained on 680,000 hours of multilingual supervised speech data.  \n",
        "The authors show that Whisper can be adapted effectively for animal sound event detection and classification across multiple species, and they provide a multi-species checkpoint.  \n",
        "As a sequence-to-sequence model, WhisperSeg predicts onset, offset, and class labels for detected calls.\n",
        "\n",
        "#### WhisperFormer\n",
        "\n",
        "In contrast to WhisperSeg’s token-generation objective, WhisperFormer directly predicts call centers and regresses onset/offset boundaries.  \n",
        "This center-based formulation is inspired by object detection and temporal action localization (TAL), especially [CenterNet] and [ActionFormer].\n",
        "\n",
        "Architecturally, WhisperFormer uses the Whisper encoder, followed by a lightweight decoder and two task-specific heads:\n",
        "- a **classification head** for class confidence,\n",
        "- a **regression head** for temporal boundaries.\n",
        "\n",
        "Following [ActionFormer], training combines [sigmoid] focal loss for classification and [DIoU] loss for regression:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{total}}(CL, S, \\tilde{CL}, \\tilde{S})\n",
        "= \\frac{1}{T_{+}}\n",
        "\\left(\n",
        "\\mathcal{L}_{\\text{class}}(CL, \\tilde{CL})\n",
        "+ \\lambda \\,\\mathbf{1}_{\\mathcal{T}_{+}} \\,\\mathcal{L}_{\\text{reg}}(S, \\tilde{S})\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "Here, \\(CL\\) and \\(\\tilde{CL}\\) denote ground-truth and predicted classes, and \\(S\\) and \\(\\tilde{S}\\) denote ground-truth and predicted temporal segments.  \n",
        "At inference time, WhisperFormer outputs per-frame class confidence scores and relative onset/offset values; final detections are produced using confidence thresholding and non-maximum suppression ([NMS])."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4a60a",
      "metadata": {},
      "source": [
        "\n",
        "#### Evaluation Metrics:\n",
        "\n",
        "In line with standard practice in detection tasks, I use the F1 score as the primary evaluation metric for all calls. A prediction is matched with a ground truth call, when their IoU is above a set threshold. Let $TP$ denote the number of true positives, $FP$ the number of false positives, $FN$ the number of false negatives, and $FC$ the number of predictions that match the temporal location of a call but are assigned an incorrect class. Then, precision and recall are defined as:\n",
        "$$precision = \\frac{TP}{TP + FP + FC}$$\n",
        "and \n",
        "$$recall = \\frac{TP}{TP+FN+FC}.$$\n",
        "Precision measures the accuracy of the predicted calls, while recall quantifies the proportion of ground truth calls correctly detected. The F1 score is the harmonic mean of precision and recall, balancing these two aspects:\n",
        "$$F1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}.$$\n",
        "\n",
        "\n",
        "##### Evaluation by Quality Classes\n",
        "To account for differences in call quality, we calculate F1 scores with respect to the ground truth quality classes. I distinguish the following metrics:\n",
        "1.  $F1_{Q1,Q2,Q3}$: F1 score calculated with respect to the ground truth labels of all quality classes 1,2 and 3. \n",
        "2. $F1_{Q1,Q2}$: F1 score calculated with respect to the ground truth labels of calls from quality classes 1 and 2. \n",
        "3. $F1_{Q1}$: F1 score calculated with respect to the ground truth labels of quality classes 1 only.\n",
        "\n",
        "Formally, for any subset of quality classes, the F1 score is computed as in ~\\ref{f1}, with precision and recall restricted to the selected quality classes.\n",
        "\n",
        "A limitation of this approach is that, when focusing on detecting high-quality calls (quality class 1), false positives from lower-quality classes (Q2 or Q3) may be less bad than completely missing detections. To address this, I define adjusted F1 metrics:\n",
        "1. $F1_{Q1,(Q1,Q3)}$: F1 score calculated with respect to the ground truth labels of quality class 1, but FP from q2 and q3 are counted as neither TPs nor FPs.\n",
        "2. $F1_{Q1,(Q2,Q3)}$: F1 score calculated with respect to the ground truth labels of quality classes 1 and 2, but FP from q3 are counted as neither TPs nor FPs.\n",
        "\n",
        "In these adjusted metrics, $TP$, $FN$, and $FC$ remain unchanged compared to $F1_{Q1}$ or $F1_{Q1,Q2}$, but the number of false positives may decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7313843e",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "First, run the below code to install lemurcalls as editable package with the dependancies from `pyproject.toml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb118193",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -e . "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce9cad5e",
      "metadata": {},
      "source": [
        "The library includes two subpackages, `lemurcalls.whisperseg` and `lemurcalls.whisperformer`, as well as tools to visualize and compare predictions produced by trained models.  \n",
        "In the following, we define demo paths to showcase the main functionality of the Python package. Since training large models typically requires a GPU, the demonstration uses a small maximum number of epochs and a single audio file for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9d749893",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = \"/projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls\"\n",
        "AUDIO_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_test\"\n",
        "AUDIO_SINGLE_DIR = \"/mnt/lustre-grete/usr/u17327/final/audios_single\"\n",
        "LABEL_TEST_DIR = \"/mnt/lustre-grete/usr/u17327/final/jsons_test\"\n",
        "\n",
        "WHISPER_BASE_PATH = f\"{PROJECT_ROOT}/whisper_models/whisper_base\"\n",
        "WHISPERSEG_TRAIN_OUT = f\"{PROJECT_ROOT}/lemurcalls/whisperseg_models\"\n",
        "WHISPERSEG_MODEL_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_ben/final_checkpoint_20251116_163404_ct2\"\n",
        "\n",
        "WHISPERFORMER_CKPT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth\"\n",
        "WHISPERFORMER_PRED_DIR = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/sc/2026-02-20_11-48-55\"\n",
        "WHISPERFORMER_VIS_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/visualization\"\n",
        "WHISPERFORMER_SNR_OUT = \"/mnt/lustre-grete/usr/u17327/final/jsons_test_filtered\"\n",
        "\n",
        "WHISPERFORMER_PREC_REC_OUT = f\"{PROJECT_ROOT}/lemurcalls/model_folder_new/final_model_20251205_030535/prec_rec\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d06596",
      "metadata": {},
      "source": [
        "## The WhisperSeg Subpackage\n",
        "\n",
        "To train a whisperseg model, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0878c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Using fixed codebook for 3 class(es): {'m': 0, 't': 1, 'w': 2, 'lt': 1, 'h': 1}\n",
            "Created 609 training samples after slicing\n",
            "epoch-000:   0%|                                        | 0/152 [00:00<?, ?it/s]/mnt/vast-nhr/home/s.dierks/u17327/micromamba/envs/lemurcalls/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "epoch-000:  26%|████████▏                      | 40/152 [03:37<10:05,  5.41s/it]^C\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.train \\\n",
        "  --initial_model_path \"{WHISPER_BASE_PATH}\" \\\n",
        "  --model_folder \"{WHISPERSEG_TRAIN_OUT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --n_threads 1 \\\n",
        "  --num_workers 1 \\\n",
        "  --max_num_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ceb55b",
      "metadata": {},
      "source": [
        "For inference, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f8bd3bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Found 1 wav files in: /mnt/lustre-grete/usr/u17327/final/audios_single\n",
            "INFO:root:Current file: [  0] /mnt/lustre-grete/usr/u17327/final/audios_single/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.WAV\n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperseg.infer \\\n",
        "  -d \"{AUDIO_SINGLE_DIR}\" \\\n",
        "  -m \"{WHISPERSEG_MODEL_DIR}\" \\\n",
        "  -o \"{WHISPERSEG_MODEL_DIR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc52565",
      "metadata": {},
      "source": [
        "For evaluation, you can use the original WhisperSeg evaluation.py or use evaluate_metrics.py (that is also applicable for WhisperFormer), that can be run via"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b5101c",
      "metadata": {},
      "outputs": [],
      "source": [
        "python -m lemurcalls.whisperseg.evaluate_metrics \\\n",
        "  --labels \"/mnt/lustre-grete/usr/u17327/final/jsons_test/U2024_09_03_10_03_14_799-U2024_09_03_10_04_42_175.UBN_v2.json\" \\\n",
        "  --predictions \"/path/to/your_prediction_file.json\" \\\n",
        "  --overlap_tolerance 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0928a3",
      "metadata": {},
      "source": [
        "Example output of `evaluate_metrics.py` for WhisperSeg model trained on high-quality data:\n",
        "```\n",
        "TP: 38\n",
        "FP: 2\n",
        "FN: 3\n",
        "FC: 1\n",
        "num gt positives: 42\n",
        "num predicted positives: 41\n",
        "Precision: 0.9268\n",
        "Recall:    0.9048\n",
        "F1-Score:  0.9157\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5daae753",
      "metadata": {},
      "source": [
        "### The WhisperFormer Subpackage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46b1abb",
      "metadata": {},
      "source": [
        "To train a WhisperFormer model, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9484e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.train \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --model_folder /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_test \\\n",
        "  --label_folder /mnt/lustre-grete/usr/u17327/final/jsons_test \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4 \\\n",
        "  --max_num_epochs 1 \\\n",
        "  --whisper_size large"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bd5395",
      "metadata": {},
      "source": [
        "For inference with a set confidence score threshold, run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acc6f70",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.infer \\\n",
        "  --checkpoint_path /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/best_model.pth \\\n",
        "  --audio_folder /mnt/lustre-grete/usr/u17327/final/audios_single \\\n",
        "  --output_dir /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/sc \\\n",
        "  --batch_size 4 \\\n",
        "  --iou_threshold 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56838a9",
      "metadata": {},
      "source": [
        "To identify a suitable confidence threshold and assess overall model behavior, compute a precision-recall curve across multiple score thresholds.  \n",
        "This helps you select a threshold that matches your objective (e.g., higher precision to reduce false positives, or higher recall to miss fewer calls).  \n",
        "The plot also highlights the threshold that achieves the highest $F_1$ score.\n",
        "\n",
        "You can control which label quality classes are considered via `eval_mode`:\n",
        "\n",
        "- `standard`: evaluates metrics for the selected quality set (e.g., $F_{1}$, $F_{1,2}$, or $F_{1,2,3}$).\n",
        "- `q3_q2`: applies the quality-aware evaluation strategy, where quality classes 2 and 3 are treated differently from class 1 (e.g.$F_{1,(2,3)})."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4fbcab",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.postprocessing.prec_rec \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_PREC_REC_OUT}\" \\\n",
        "  --overlap_tolerance 0.3 \\\n",
        "  --allowed_qualities 1 \\\n",
        "  --eval_mode standard \\"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca71a90",
      "metadata": {},
      "source": [
        "![PrecRec](precision_recall_curve.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8137b8a",
      "metadata": {},
      "source": [
        "## Thresholds and visualization\n",
        "\n",
        "If the aim is to only detect high-quality calls from the focal animal - as in our application where the goal is in the future to equip each individual with its own collar - it can be usefull to apply postprocessing filters, such as Signal-to_Noise Ratio (SNR) and amplitude filters.\n",
        "\n",
        "To determine suitable thresholds for your dataset, you can plot the SNR and maximum amplitude of calls in the training set and color the points by quality class.\n",
        "You can also color the points by the confidence scores produced by a final trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967f77df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-27_11-17-53/run_arguments.json\n",
            "Checkpoint: whisper_size=large, num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "^C                               \n"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.whisperformer.visualization.scatterplot_ampl_snr_score \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\" \\\n",
        "  --num_classes 3 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed3ff16",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Quality)](scatter_snr_vs_amplitude.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b49095",
      "metadata": {},
      "source": [
        "We can see in our dataset when we plot SNR and maximal amplitude of the labeled calls, that quality class 2 and quality class 3 cannot be (linearly) seperated easily. Quality class 1 on the other hand can - with the chosen thresholds - be seperated rather well from the other quality classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6dc3c9",
      "metadata": {},
      "source": [
        "![SNR vs Amplitude (Model Score)](scatter_snr_vs_amplitude_model_score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec937209",
      "metadata": {},
      "source": [
        "To filter existing predictions by SNR and maximal amplitude use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b996781",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m lemurcalls.whisperformer.postprocessing.filter_labels_by_snr \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_SNR_OUT}\" \\\n",
        "  --snr_threshold -1 \\\n",
        "  --amplitude_threshold 0.035"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e866d0f",
      "metadata": {},
      "source": [
        "## Visualization of model outputs\n",
        "To visualize, analyze and compare the predictions of two trained models, you can use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a865e906",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments saved to: /projects/extern/CIDAS/cidas_digitalisierung_lehre/mthesis_sophie_dierks/dir.project/lemurcalls/lemurcalls/model_folder_new/final_model_20251205_030535/visualization/2026-02-26_22-51-18/run_arguments.json\n",
            "Checkpoint: num_decoder_layers=1, num_head_layers=2, num_classes=3\n",
            "Detected Whisper size: large\n",
            "Model loaded (Whisper large)\n",
            "                                 \r"
          ]
        }
      ],
      "source": [
        "!python -m lemurcalls.visualize_predictions \\\n",
        "  --checkpoint_path \"{WHISPERFORMER_CKPT}\" \\\n",
        "  --audio_folder \"{AUDIO_TEST_DIR}\" \\\n",
        "  --label_folder \"{LABEL_TEST_DIR}\" \\\n",
        "  --pred_label_folder \"{WHISPERFORMER_PRED_DIR}\" \\\n",
        "  --output_dir \"{WHISPERFORMER_VIS_OUT}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3fb1c7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "071c1fac",
      "metadata": {},
      "source": [
        "The audio recordings are split into 30-second segments. For each segment, a figure with the following four rows is generated:\n",
        "\n",
        "1. Mel spectrogram computed by the original Whisper encoder.  \n",
        "2. WhisperFormer confidence scores for each spectrogram column and call class; final predictions above the selected threshold (red dashed line) are shown rectangles colored by predicted call class.  \n",
        "3. Ground-truth labels colored by call class, together with their assigned quality classes.  \n",
        "4. WhisperSeg predictions colored by predicted call class.\n",
        "\n",
        "The first two examples below were generated using models trained only on high-quality calls (Q1), while the remaining examples were generated using models trained on all quality classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7998a3",
      "metadata": {},
      "source": [
        "![Spec2](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_01_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b00ebf0",
      "metadata": {},
      "source": [
        "The visualization above highlights a common WhisperFormer issue: because NMS is applied per class, and because `moan` and `wail` are inherently difficult to distinguish, both class scores may exceed the threshold for the same event. This can lead to duplicate predictions for a single ground-truth call.  \n",
        "For WhisperSeg, this example shows that very short calls can also be detected even when they are not present in the ground-truth labels, likely due to the higher temporal resolution of WhisperSeg spectrograms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa1809",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b02aa952",
      "metadata": {},
      "source": [
        "![Spec4](U2025_09_24_07_58_20_714-U2025_09_24_07_59_48_091.UBN_v4_segment_02_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0269fb",
      "metadata": {},
      "source": [
        "The above visualization is an example of good preformances of both models. WhisperSeg finds an "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0c042f",
      "metadata": {},
      "source": [
        "![Spec6](U2024_09_24_12_06_37_702-U2024_09_24_12_08_05_079.UBN_v2_segment_00_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62356ef8",
      "metadata": {},
      "source": [
        "Here, we can see that when trained on all data, WhisperFormer is more sensitive than WhisperSeg. Further, calls labeled as quality classes 1 and 2 receive higher WhisperFormer confidence scores than calls from quality class 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b791e0d9",
      "metadata": {},
      "source": [
        "![Spec9](U2024_09_24_12_24_06_562-U2024_09_24_12_25_33_938.UBN_v2_segment_00_spectrogram_scores_gt_all.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e20e1ef",
      "metadata": {},
      "source": [
        "The visualization above shows that both models struggle to correctly detect highly overlapping calls. WhisperFormer confidence scores suggest that they contain additional information that could potentially be used to refine the final onset and offset predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561f88fe",
      "metadata": {},
      "source": [
        "![Spec5](U2025_09_24_12_59_26_005-U2025_09_24_13_00_54_382.UBN_v1_segment_02_spectrogram_scores_gt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84924ef",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "With this codebase, we showed that both WhisperSeg and WhisperFormer can be trained to automatically detect, segment, and classify lemur calls in long audio recordings, yielding promising results—especially when trained on high-quality data only.  \n",
        "Using dedicated visualizations, we identified suitable SNR and amplitude thresholds to improve model performance and used precision-recall curves to select an appropriate confidence threshold for WhisperFormer. Additional visual analyses were used to compare model behavior: when trained on data from all quality classes, WhisperFormer appears more sensitive than WhisperSeg, but it also tends to detect very faint calls and occasional background noise.  \n",
        "These visualizations also helped identify specific failure modes, such as duplicate predictions of two call classes for the same ground-truth call."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e0832d",
      "metadata": {},
      "source": [
        "### Refernces\n",
        "\n",
        "[WhisperSeg] Gu, N., Lee, K., Basha, M., Ram, S. K., You, G., & Hahnloser, R. H. (2024, April). Positive transfer of the whisper speech transformer to human and animal voice activity detection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7505-7509). IEEE.\n",
        "[ActionFormer] Zhang, C. L., Wu, J., & Li, Y. (2022, October). Actionformer: Localizing moments of actions with transformers. In European Conference on Computer Vision (pp. 492-510). Cham: Springer Nature Switzerland.\n",
        "[Whisper] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023, July). Robust speech recognition via large-scale weak supervision. In International conference on machine learning (pp. 28492-28518). PMLR.\n",
        "[NMS] Hosang, J., Benenson, R., & Schiele, B. (2017). Learning non-maximum suppression. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4507-4515).\n",
        "[CenterNet] Zhou, X., Wang, D., & Krähenbühl, P. (2019). Objects as points. arXiv preprint arXiv:1904.07850.\n",
        "[MAcedonia] Macedonia, J. M. (1993). The vocal repertoire of the ringtailed lemur (Lemur catta). Folia primatologica, 61(4), 186-217.\n",
        "[sigmoid] Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).\n",
        "[DIoU] Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., & Ren, D. (2020, April). Distance-IoU loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 07, pp. 12993-13000)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4a6d90",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
